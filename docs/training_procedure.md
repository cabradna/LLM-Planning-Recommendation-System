# Detailed Training Procedure: Dyna-Q Job Recommender for a Single Applicant (Conceptual Description)

## 1. Introduction & Training Philosophy

This document provides a detailed conceptual description of the training procedure for the Dyna-Q based job recommendation agent. The central philosophy is to learn an optimal recommendation policy specialized **for a single target applicant** by simulating interactions over multiple training episodes.

**Single Applicant Focus:**
The training process centers entirely around one chosen applicant. Their specific profile information, represented as a state vector ($v_{applicant\_state}$) fetched from MongoDB, remains constant throughout the training run. The goal is to tune the agent's Q-network ($Q(s, a; \theta)$) and World Model ($\hat{P}(R | s, a)$) to accurately reflect the values and dynamics associated with recommending jobs *to this specific applicant*. This approach allows for deep personalization. Generalizing the learned policy to other applicants is reserved for future work.

**Comparative Strategies:**
This procedure will be executed independently using three different methods for generating the crucial reward signal, enabling a comparative analysis:
1.  **Cosine Similarity Rewards:** Rewards based on embedding similarity between the applicant and the job.
2.  **LLM Feedback Only Rewards:** Rewards derived from an LLM simulating the applicant's behavior.
3.  **Hybrid (Cosine Pre-training + LLM Fine-tuning):** A two-phase approach starting with Cosine rewards and refining with LLM rewards.

**Framework:**
The training methodology utilizes the **Dyna-Q** algorithm, a model-based Reinforcement Learning approach that integrates three key learning processes:
* **Direct Reinforcement Learning:** Learning directly from interactions with the simulated environment (where rewards are generated by the active strategy).
* **Model Learning:** Learning a predictive model of the environment's reward dynamics.
* **Planning:** Leveraging the learned model to simulate experiences and perform additional updates, improving learning efficiency.

## 2. Prerequisites and Setup

Before training commences, the following setup steps are necessary:

* **Target Applicant Selection:** Identify the specific applicant (`target_applicant_id`) for whom the policy will be optimized.
* **Database Connection:** Establish a `pymongo` connection to the MongoDB Atlas cluster containing the `rl_jobsdb` database.
* **Applicant State Vector ($v_{applicant\_state}$):** Fetch the target applicant's pre-computed embeddings (e.g., hard/soft skills) from the `candidates_embeddings` collection. Aggregate these into a fixed PyTorch tensor, `v_applicant_state`, which represents the constant state $s$ for this training run.
* **Neural Network Initialization:**
    * Instantiate the **Q-Network** ($Q(s, a; \theta)$) and **World Model** ($\hat{P}(R | s, a)$) MLP architectures. Initialize their weights ($\theta$ for Q-Network, separate weights for World Model) randomly (e.g., Xavier/Kaiming initialization), unless starting Phase 2 of the Hybrid strategy, where weights are loaded from the end of Phase 1.
    * Create the **Target Q-Network** ($Q(s, a; \theta^{-})$) as an identical copy of the Q-Network, initializing its weights $\theta^{-}$ with the initial $\theta$.
* **Replay Buffer ($\mathcal{D}_{real}$):** Initialize an empty replay buffer with a fixed maximum capacity to store *real* interaction experiences. Since the state is static ($s_{t+1}=s_t$), tuples can be stored as $(s_t, a_t, R_{t+1})$.
* **Optimizers:** Define separate optimizers (e.g., Adam) for the Q-Network and the World Model, configured with appropriate learning rates.
* **Hyperparameters:** Set values for key training parameters: discount factor ($\gamma$), learning rates, exploration parameters ($\epsilon_{start}, \epsilon_{end}, \epsilon_{decay}$), replay buffer capacity, mini-batch size, number of planning steps ($N$) per real step, target network update frequency (C), total number of training episodes, maximum steps per episode, and the number of candidate jobs ($n$) to sample at each step.

## 3. Core Training Loop: Conceptual Flow (Dyna-Q for Single Applicant)

Training iterates over a set number of episodes. Each episode represents a simulated sequence of job recommendation interactions for the target applicant.

**Within each episode:**

The process iterates through a sequence of time steps, up to a maximum limit per episode. At each time step $t$:

1.  **Action Selection (Policy Execution):**
    * Determine whether to explore or exploit based on the current exploration rate $\epsilon$ (which typically decays over the course of training).
    * **If Exploring (with probability $\epsilon$):** Randomly sample a set of `n` candidate job IDs. Select one job ID randomly as the action $a_t$. Fetch its corresponding vector $v_{job,t}$ from the `job_embeddings` collection in MongoDB.
    * **If Exploiting (with probability $1-\epsilon$):**
        * Sample a set of `n` candidate job IDs (potentially using heuristics based on the static $v_{applicant\_state}$ to focus the search).
        * Fetch the corresponding job vectors $v_{job}$ for all `n` candidates from `job_embeddings`.
        * Create `n` input tensors by concatenating the fixed $v_{applicant\_state}$ with each candidate $v_{job}$.
        * Pass these inputs through the **main Q-Network ($Q(s, a; \theta)$)** to get estimated Q-values for all candidate actions.
        * Select the action $a_t$ (the job ID) corresponding to the candidate job with the highest Q-value. Let its vector be $v_{job,t}$.

2.  **Environment Interaction & Reward Generation:**
    * The chosen action $a_t$ (represented by $v_{job,t}$) is conceptually "taken" in the fixed state $s_t$ (represented by $v_{applicant\_state}$).
    * The immediate reward $R_{t+1}$ is generated based on the **active training strategy**:
        * **Cosine Strategy:** Calculate $R_{t+1} = \text{cosine\_similarity}(v_{applicant\_state}, v_{job, t})$.
        * **LLM Feedback Strategy:** Input $s_t$ and $a_t$ (job details) to the LLM simulator. Map the simulator's output action (e.g., 'APPLY') to the reward value $R_{t+1}$.
        * **Hybrid Strategy:** Use the reward calculation method corresponding to the current training phase (Cosine for Phase 1, LLM for Phase 2).
    * Determine the next state $s_{t+1}$. Since the state is static in this phase, $s_{t+1} = s_t$.

3.  **Experience Storage:**
    * Store the resulting experience tuple $(s_t, a_t, R_{t+1})$ in the replay buffer $\mathcal{D}_{real}$. Note that $s_t$ refers to the fixed `v_applicant_state`, and $a_t$ refers to the chosen $v_{job,t}$.

4.  **Direct RL Update (Learning from Real Experience):**
    * Check if the replay buffer $\mathcal{D}_{real}$ contains enough experiences to sample a mini-batch.
    * If yes, randomly sample a mini-batch of experiences $(s_j, a_j, R_{j+1})$ from $\mathcal{D}_{real}$.
    * For each experience in the mini-batch:
        * Calculate the **target Q-value ($y_j$)**:
            $$ y_j = R_{j+1} + \gamma \max_{a'} Q(s_{j+1}, v_{job}'; \theta^{-}) $$
            This involves:
            * Using the sampled reward $R_{j+1}$.
            * Determining the next state $s_{j+1}$ (which is $s_j$ here).
            * Evaluating *all candidate actions* $a'$ for the next state $s_{j+1}$ using the **target Q-Network ($\theta^{-}$)**. This requires sampling candidate jobs, fetching their vectors $v_{job}'$, and finding the maximum predicted Q-value $\max_{a'} Q(s_{j+1}, v_{job}'; \theta^{-})$.
        * Calculate the Q-value prediction from the **main Q-Network ($\theta$)**: $Q(s_j, a_j; \theta)$.
        * Compute the MSE loss between the prediction and the target: $(Q(s_j, a_j; \theta) - y_j)^2$.
    * Average the loss over the mini-batch.
    * Perform backpropagation and update the weights $\theta$ of the **main Q-Network** using its optimizer.

5.  **Model Learning (Updating the World Model):**
    * Check if enough experiences are available for a batch.
    * Sample a mini-batch of *real* experiences $(s_j, a_j, R_{j+1})$ from $\mathcal{D}_{real}$.
    * For each experience:
        * Use the **World Model ($\hat{P}$)** to predict the reward $\hat{R}_j$ given the input $[s_j, a_j]$.
        * Calculate the supervised learning loss (e.g., MSE) between the predicted reward $\hat{R}_j$ and the actual observed reward $R_{j+1}$.
    * Average the loss over the mini-batch.
    * Perform backpropagation and update the weights of the **World Model** using its optimizer. The model learns to predict the reward dynamics dictated by the *active training strategy*.

6.  **Planning (Learning from Simulated Experience):**
    * Perform $N$ planning steps. For each step:
        * Randomly sample a *previously experienced* state-action pair $(s, a)$ from memory (e.g., from $\mathcal{D}_{real}$). Let the corresponding vectors be $v_{applicant\_state}$ (which is fixed) and $v_{job}$.
        * Use the **learned World Model ($\hat{P}$)** to predict the simulated reward $\hat{R}$ for this $(s, a)$ pair. The predicted next state $\hat{s}'$ is simply $s$ (the fixed $v_{applicant\_state}$).
        * Calculate a **simulated target Q-value ($y_{sim}$)**:
            $$ y_{sim} = \hat{R} + \gamma \max_{a'} Q(\hat{s}', v_{job}'; \theta^{-}) $$
            This uses the *model's predicted reward* $\hat{R}$ and the *target Q-Network* ($\theta^{-}$) to estimate the value from the (unchanged) next state $\hat{s}'$.
        * Calculate the Q-value prediction from the **main Q-Network ($\theta$)**: $Q(s, a; \theta)$.
        * Compute the MSE loss between the prediction and the simulated target: $(Q(s, a; \theta) - y_{sim})^2$.
        * Perform backpropagation and update the weights $\theta$ of the **main Q-Network** using this loss derived from simulated experience. This step allows the agent to refine its Q-values based on the learned model without requiring new "real" interactions.

7.  **Target Network Update:**
    * Periodically (every C steps), copy the weights $\theta$ from the main Q-Network to the target Q-Network ($\theta^{-}$).

8.  **Loop Continuation/Termination:** Proceed to the next time step ($t+1$) or terminate the episode if the maximum step limit is reached or another termination condition applies.

**After completing all episodes:** The training run for the specific applicant and strategy is finished. The learned Q-Network $Q(s, a; \theta)$ represents the optimized policy for that applicant under that reward regime.

## 4. Convergence and Termination Criteria

* Training typically runs for a predetermined `NUM_EPISODES`.
* Convergence can be monitored by observing the stabilization of metrics like average episode reward or loss values over a window of recent episodes.
* The exploration rate $\epsilon$ is gradually reduced during training to shift focus from exploration to exploitation.

## 5. Evaluation During Training

* While the primary evaluation compares the final outcomes of different strategies, monitoring metrics *during* training for the single applicant is essential:
    * **Total Reward per Episode:** Tracks overall improvement in achieving rewards.
    * **Average Q-Loss per Episode:** Indicates how well the Q-network is fitting the target values.
    * **Average Model Loss per Episode:** Shows how well the world model is predicting rewards.
* Plotting these metrics helps understand the learning dynamics for the specific applicant under the current strategy and diagnose issues like instability or lack of convergence.

This conceptual description outlines the detailed steps and theoretical underpinnings of the Dyna-Q training procedure tailored for a single applicant, emphasizing the role of the different neural networks, the interaction with MongoDB, and the distinct reward generation strategies being compared.