# -*- coding: utf-8 -*-
"""RL_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xtjRuz2p53tpTvcXltKIXwcJP8kxUZuy
"""

# -*- coding: utf-8 -*-
# Header Placeholder for Future Use :)
"""Mistral_7B_Inference_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/YOUR_GITHUB_USERNAME/YOUR_REPO/blob/main/Mistral_7B_Inference_Colab.ipynb
    (You can save this to your GitHub or Google Drive from Colab)

# Running Mistral 7B Inference in Google Colab

This notebook demonstrates how to load and run inference with the Mistral-7B-Instruct-v0.2 model using the Hugging Face `transformers` library. We will use 4-bit quantization to make the model fit within the available GPU memory on Google Colab (typically a T4 GPU).

**Make sure you have enabled the GPU runtime!** (Runtime -> Change runtime type -> Hardware accelerator: GPU)
"""

"""# 1. Checking GPU Availability"""

import torch

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU runtime. Go to Runtime > Change runtime type > Hardware accelerator > GPU.')
else:
  print(gpu_info)

# Check if CUDA is available
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device name: {torch.cuda.get_device_name(0)}")
    print(f"CUDA device capability: {torch.cuda.get_device_capability(0)}")
    device = "cuda:0"
else:
    device = "cpu"
    print("WARNING: CUDA not available. Inference will be extremely slow on CPU.")

"""# 2. Installing Necessary Libraries"""

# Install the transformers, accelerate, and bitsandbytes libraries.
# accelerate helps with efficiently loading models, and bitsandbytes is needed for 4-bit quantization.
!pip install -q -U transformers accelerate bitsandbytes scipy

"""# 3. Loading Model"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import time

# @title 4. Load Model and Tokenizer (with 4-bit quantization)

model_id = "mistralai/Mistral-7B-Instruct-v0.2" # We'll use the instruct version for better chat capabilities

# Configure 4-bit quantization
# bnb_4bit_compute_dtype="float16" is recommended for NVIDIA GPUs >= Volta
# bnb_4bit_quant_type="nf4" is the default and generally recommended
# use_double_quant=True applies double quantization for slightly better accuracy at minimal overhead
# bnb_4bit_use_nested_quant=True is another name for use_double_quant - using the explicit one is fine
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_nested_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 if your GPU supports it (T4 does), otherwise float16
)

print(f"Loading tokenizer for model: {model_id}")
tokenizer = AutoTokenizer.from_pretrained(model_id)

print(f"Loading model {model_id} with 4-bit quantization...")
start_time = time.time()

# Load the model. device_map="auto" is useful as it automatically distributes the model
# across available GPUs or CPU if parts don't fit on GPU.
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)

end_time = time.time()
print(f"Model loaded in {end_time - start_time:.2f} seconds.")
print(f"Model moved to device: {model.device}") # Should show cuda:0 or similar

"""# 5. Prompting/Inferencing"""

# @title Roleplay Prompts

# 7. Load Data (Example structure)
applicant_data = {
    "bio": "Experienced software engineer with a focus on Python and machine learning...",
    "resume": "Education:\n- BS Computer Science...\nExperience:\n- Sr. Software Engineer at X...\nSkills:\n- Python, TensorFlow, PyTorch, SQL, AWS..."
}

# Example job data - replace with your actual data loading
# LOAD FROM MONGODB
all_job_postings = [
    {"title": "Senior Python Engineer", "description": "We are looking for a senior Python engineer..."},
]

# 8. Define Prompt Building Functions
def build_title_prompt(applicant_profile, job_title):
    prompt = f"""### ROLE ###
      You are role-playing as a job applicant. Your persona is defined by the profile below.

      ### APPLICANT PROFILE ###
      {applicant_profile['bio']}

      ---
      {applicant_profile['resume']}

      ### SIMULATION INSTRUCTION ###
      You are evaluating job postings. You will be shown a Job Title and must decide whether to 'CLICK' to view the full job description or 'IGNORE' the job based *only* on how well the title matches your profile and interests.

      Your response MUST follow the format below. Provide a brief reason for your decision.

      ### JOB TITLE ###
      {job_title}

      ### YOUR DECISION ###
      DECISION: [CLICK or IGNORE]
      REASON: [Brief explanation based on the title and your profile]
      """
    return prompt

def build_description_prompt(applicant_profile, job_title, job_description):
    prompt = f"""### ROLE ###
      You are role-playing as a job applicant. Your persona is defined by the profile below.

      ### APPLICANT PROFILE ###
      {applicant_profile['bio']}

      ---
      {applicant_profile['resume']}

      ### SIMULATION INSTRUCTION ###
      You previously clicked on a job posting based on its title. Now you have the full job description. Based on your profile, the job title, and the full description, you must decide whether to 'APPLY' for this job, 'SAVE' it for later consideration, or 'IGNORE' it because it's not a good fit after all.

      Your response MUST follow the format below. Provide a brief reason for your decision.

      ### JOB TITLE ###
      {job_title}

      ### JOB DESCRIPTION ###
      {job_description}

      ### YOUR DECISION ###
      DECISION: [APPLY or SAVE or IGNORE]
      REASON: [Brief explanation based on the job description and your profile]
      """
    return prompt

# @title 5.1 Define LLM Interaction Function
def get_llm_decision(prompt, model, tokenizer, device, max_new_tokens=50):
    """Sends prompt to LLM and parses the DECISION and REASON."""
    messages = [{"role": "user", "content": prompt}]
    encoded_input = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(device)

    # Generate a short response as we only need the decision and reason
    generated_ids = model.generate(
        encoded_input,
        max_new_tokens=max_new_tokens,
        do_sample=False, # Use greedy decoding for more predictable format
        pad_token_id=tokenizer.eos_token_id # Avoid potential issues with padding in short generations
    )
    decoded_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    # Find the section after your prompt where the model's response starts
    # This part might need tweaking based on how the chat template and model behave
    try:
        response_start_index = decoded_output.find("[/INST]") + len("[/INST]")
        llm_response_text = decoded_output[response_start_index:].strip()
    except:
        llm_response_text = decoded_output # Fallback

    # --- Parse the output ---
    decision = "PARSE_ERROR"
    reason = "N/A"
    decision_line = None

    # Split response into lines to find the DECISION line
    lines = llm_response_text.split('\n')
    for line in lines:
        if line.startswith("DECISION:"):
            decision_line = line.replace("DECISION:", "").strip()
            # Simple parsing assuming it's the first word after DECISION:
            decision = decision_line.split()[0].upper() if decision_line else "PARSE_ERROR"
            break # Assume the first DECISION line is the one we want

    # Find the REASON line
    reason_line_start = llm_response_text.find("REASON:")
    if reason_line_start != -1:
         # Extract everything after REASON: and potentially trim
        reason = llm_response_text[reason_line_start + len("REASON:"):].strip()
         # If DECISION line was found, trim the reason text to end before the DECISION line if it comes after
        if decision_line and llm_response_text.find("DECISION:") < reason_line_start:
             # This case is unlikely with the prompt structure, but defensive programming
             pass # Reason comes after decision, no need to trim relative to decision
        elif decision_line and reason_line_start < llm_response_text.find("DECISION:"):
             # If REASON appears before DECISION (unexpected but possible LLM output)
             pass # Need more robust parsing if this happens often

    # Basic check for expected decisions
    valid_decisions_step1 = ["CLICK", "IGNORE"]
    valid_decisions_step2 = ["APPLY", "SAVE", "IGNORE"]

    # Determine which step's decisions to validate against (simple heuristic based on prompt content)
    if "JOB DESCRIPTION" in prompt:
        valid_decisions = valid_decisions_step2
    else:
        valid_decisions = valid_decisions_step1

    if decision not in valid_decisions:
         print(f"WARNING: LLM returned unexpected decision format: {decision}. Raw output:\n{llm_response_text}\n")
         decision = "PARSE_ERROR" # Mark as error if not in expected set


    return decision, reason, llm_response_text # Return parsed decision, reason, and raw output for debugging

# @title 5.2 Simulation Loop (Example Basic Iteration)

# Initialize RL Agent (This is where your RL agent object/logic would go)
# For now, we'll just simulate the loop and print results

# Example: Process the first 5 jobs
num_jobs_to_process = min(5, len(all_job_postings))
simulation_data = [] # Store simulation steps for RL training later

print("\n--- Starting Simulation ---")

for i in range(num_jobs_to_process):
    current_job = all_job_postings[i] # In real RL, this would come from agent's policy
    job_title = current_job['title']
    job_description = current_job['description'] # Load full description here for step 2 if needed

    print(f"\nProcessing Job {i+1}/{num_jobs_to_process}: {job_title}")

    # --- Step 1: Title Only Decision (CLICK/IGNORE) ---
    prompt1 = build_title_prompt(applicant_data, job_title)
    decision1, reason1, raw_output1 = get_llm_decision(prompt1, model, tokenizer, device, max_new_tokens=50)

    print(f"  Title Decision: {decision1}")
    print(f"  Title Reason: {reason1}")
    # print(f"  Raw LLM output 1:\n{raw_output1}\n---") # Uncomment for debugging

    simulation_step = {
        "job_index": i,
        "job_title": job_title,
        "step": 1,
        "llm_input_prompt": prompt1,
        "llm_raw_output": raw_output1,
        "llm_decision": decision1,
        "llm_reason": reason1,
        "job_description_provided": False # Flag to indicate state
    }
    simulation_data.append(simulation_step)


    if decision1 == "CLICK":
        print(f"  LLM clicked. Retrieving full description...")

        # --- Step 2: Full Description Decision (APPLY/SAVE/IGNORE) ---
        prompt2 = build_description_prompt(applicant_data, job_title, job_description)
        decision2, reason2, raw_output2 = get_llm_decision(prompt2, model, tokenizer, device, max_new_tokens=100) # Allow more tokens for reason

        print(f"  Description Decision: {decision2}")
        print(f"  Description Reason: {reason2}")
        # print(f"  Raw LLM output 2:\n{raw_output2}\n---") # Uncomment for debugging

        simulation_step_2 = {
            "job_index": i,
            "job_title": job_title,
            "step": 2,
            "job_description": job_description, # Include description in data for step 2
            "llm_input_prompt": prompt2,
            "llm_raw_output": raw_output2,
            "llm_decision": decision2,
            "llm_reason": reason2,
             "job_description_provided": True
        }
        simulation_data.append(simulation_step_2)

    # --- RL Agent Learn/Act Step (Placeholder) ---
    # Your RL agent would process the simulation_step(s) here.
    # It might update its Q-values, policy, etc., based on the observed
    # state and the LLM's action (which is treated as the "correct" or
    # "demonstrated" action for this state).
    # The agent might then decide which job to present next, potentially
    # focusing on types of jobs it's less certain about based on the LLM's behavior.
    # For this simple loop, we just move to the next job in the list.
    pass # RL learning step goes here

print("\n--- Simulation Complete ---")

# 'simulation_data' now contains the logged interactions, which you can use
# to train your RL agent. Each entry logs the state (job info), the LLM's decision (action),
# and the reasoning. You might need to add a 'reward' signal to this data
# based on some external criteria (e.g., a human labeling some decisions, or a heuristic).

# Example of how the collected data looks (print first few steps)
# import json
# print(json.dumps(simulation_data[:5], indent=2))