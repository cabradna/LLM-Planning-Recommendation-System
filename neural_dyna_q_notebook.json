{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10"
        },
        "colab": {
            "provenance": []
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Neural Dyna-Q for Job Recommendation: Model-Based RL Approach\n",
                "\n",
                "**Author:** LLM Lab Team  \n",
                "**Date:** 2023-10-15"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Introduction and Overview\n",
                "\n",
                "This notebook demonstrates a Neural Dyna-Q approach to personalized job recommendation. By combining deep reinforcement learning with model-based planning, we build a system that learns to recommend jobs that match candidate skills and preferences.\n",
                "\n",
                "### 1.1 Core Algorithm Concepts\n",
                "\n",
                "The Neural Dyna-Q algorithm integrates these key components:\n",
                "\n",
                "- **Q-Learning**: Learning a value function that predicts the utility of job recommendations\n",
                "- **World Model**: A neural network that models the environment dynamics\n",
                "- **Planning**: Using the world model to simulate experiences and improve the value function\n",
                "- **Exploration Strategy**: Balancing exploration and exploitation with epsilon-greedy policy\n",
                "\n",
                "### 1.2 Key Features of Implementation\n",
                "\n",
                "- **Single-Applicant Specialization**: Each agent is trained for one specific applicant\n",
                "- **Neural Networks**: Deep learning for value function and environment dynamics approximation\n",
                "- **Multiple Reward Generation Strategies**: Cosine similarity, LLM feedback, and hybrid approaches\n",
                "- **Dyna-Q Algorithm**: Combines direct RL with model-based planning\n",
                "\n",
                "### 1.3 Expected Outcomes\n",
                "\n",
                "By the end of this notebook, we'll have:\n",
                "1. A trained job recommendation agent for a specific candidate\n",
                "2. Visualizations of the learning process\n",
                "3. A list of personalized job recommendations\n",
                "4. Insights into the model's performance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Environment Setup\n",
                "\n",
                "First, we'll set up our environment, install necessary packages, and import libraries. If running in Google Colab, we'll clone the repository and install dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if running in Google Colab\n",
                "# Import necessary libraries for setup\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuration settings for the setup process, particularly for Colab environments.\n",
                "SETUP_PATH_CONFIG = {\n",
                "    \"repo_url\": \"https://github.com/cabradna/LLM-Planning-Recommendation-System.git\"  # URL of the repository to clone.\n",
                "}\n",
                "\n",
                "# Configuration for enabling specific strategies during setup (e.g., LLM, hybrid).\n",
                "SETUP_STRATEGY_CONFIG = {\n",
                "    \"llm\": {\"enabled\": True},  # Enable LLM-related dependency installation.\n",
                "    \"hybrid\": {\"enabled\": False} # Enable hybrid strategy dependency installation.\n",
                "}\n",
                "\n",
                "# Determine if the code is running in Google Colab.\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"Running in Google Colab. Cloning repository and installing dependencies...\")\n",
                "\n",
                "    try:\n",
                "        # Retrieve repository URL from configuration.\n",
                "        repo_url = SETUP_PATH_CONFIG[\"repo_url\"]\n",
                "        repo_name = Path(repo_url).stem  # Extract repository name from the URL.\n",
                "\n",
                "        # Clone the repository.\n",
                "        print(f\"Cloning repository: {repo_url}\")\n",
                "        subprocess.run([\"git\", \"clone\", \"-q\", repo_url], check=True)  # Clone quietly and raise an error if cloning fails.\n",
                "\n",
                "        # Navigate into the cloned repository directory.\n",
                "        cloned_repo_path = Path(repo_name)\n",
                "        if cloned_repo_path.is_dir():\n",
                "            os.chdir(cloned_repo_path)\n",
                "            print(f\"Changed directory to: {os.getcwd()}\")\n",
                "        else:\n",
                "            raise FileNotFoundError(f\"Cloned repository directory '{repo_name}' not found or is not a directory.\")\n",
                "\n",
                "        # Install base requirements from requirements.txt.\n",
                "        requirements_path = Path(\"requirements.txt\")\n",
                "        if requirements_path.is_file():\n",
                "            print(\"Installing base requirements from requirements.txt...\")\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_path)], check=True)  # Use the correct pip and install quietly.\n",
                "        else:\n",
                "            print(\"Warning: requirements.txt not found. Skipping base requirements installation.\")\n",
                "\n",
                "        # Install the project package in editable mode.\n",
                "        print(\"Attempting to install project package in editable mode...\")\n",
                "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"], check=True)  # Install the current package in editable mode.\n",
                "\n",
                "        # Install extra packages if LLM or hybrid strategy is enabled.\n",
                "        if SETUP_STRATEGY_CONFIG[\"llm\"][\"enabled\"] or SETUP_STRATEGY_CONFIG[\"hybrid\"][\"enabled\"]:\n",
                "            print(\"Installing LLM-related packages (transformers, accelerate, bitsandbytes)...\")\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"accelerate\"], check=True)\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bitsandbytes>=0.45.0\"], check=True)\n",
                "            print(\"LLM-related packages installed successfully.\")\n",
                "\n",
                "        print(\"Repository cloned and dependencies installed successfully.\")\n",
                "\n",
                "        # Add the project root to the Python path.\n",
                "        project_root = Path(os.getcwd()).absolute()\n",
                "        if str(project_root) not in sys.path:\n",
                "            sys.path.insert(0, str(project_root))  # Insert at the beginning to prioritize project modules.\n",
                "            print(f\"Added {project_root} to sys.path\")\n",
                "\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"Setup command failed: {e}\")\n",
                "        print(\"Please check the repository URL, requirements file, setup.py, and Colab environment.\")\n",
                "        raise  # Re-raise the exception to halt execution.\n",
                "    except FileNotFoundError as e:\n",
                "        print(f\"Setup error: {e}\")\n",
                "        raise  # Re-raise the exception to halt execution.\n",
                "else:\n",
                "    print(\"Not running in Google Colab. Assuming local environment is set up.\")\n",
                "\n",
                "    try:\n",
                "        # Determine the project root directory.\n",
                "        project_root = Path(__file__).resolve().parent.parent\n",
                "    except NameError:\n",
                "        # Handle cases where __file__ is not defined (e.g., interactive environments).\n",
                "        project_root = Path('.').absolute()\n",
                "        print(f\"Warning: __file__ not defined. Assuming project root is CWD: {project_root}\")\n",
                "\n",
                "    # Add the project root to the Python path if it's not already there.\n",
                "    if str(project_root) not in sys.path:\n",
                "        sys.path.insert(0, str(project_root))\n",
                "        print(f\"Added {project_root} to sys.path for local execution.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard imports for data manipulation, visualization, and deep learning\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import random\n",
                "from tqdm.notebook import tqdm  # Use tqdm.notebook for Colab/notebook progress bars\n",
                "import torch\n",
                "from pathlib import Path\n",
                "\n",
                "# Dynamically add the project root to the Python path\n",
                "# This allows importing modules from the project's source code, regardless of the current working directory.\n",
                "try:\n",
                "    # Determine the project root directory based on the location of this script.\n",
                "    # Assumes the script is located in a subdirectory of the project root.\n",
                "    project_root = Path(__file__).resolve().parent.parent\n",
                "except NameError:\n",
                "    # Handle cases where __file__ is not defined, such as in interactive environments.\n",
                "    project_root = Path('.').absolute()\n",
                "\n",
                "# Add the project root to the Python path if it's not already there.\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))  # Prioritize project modules over installed ones\n",
                "\n",
                "# Import configuration settings from the project's config module\n",
                "# These settings define various aspects of the experiment, such as database connections,\n",
                "# model architectures, training parameters, and evaluation metrics.\n",
                "from config.config import (\n",
                "    DB_CONFIG,\n",
                "    MODEL_CONFIG,\n",
                "    TRAINING_CONFIG,\n",
                "    STRATEGY_CONFIG,\n",
                "    PATH_CONFIG,\n",
                "    EVAL_CONFIG,\n",
                "    HF_CONFIG\n",
                ")\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "# This ensures that the experiment produces consistent results across multiple runs,\n",
                "# which is essential for debugging and comparing different approaches.\n",
                "random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
                "np.random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
                "torch.manual_seed(TRAINING_CONFIG[\"random_seed\"])\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(TRAINING_CONFIG[\"random_seed\"])\n",
                "\n",
                "# Determine the device to use for PyTorch computations (CPU or GPU)\n",
                "# If a GPU is available, it will be used to accelerate training.\n",
                "device = torch.device(TRAINING_CONFIG[\"device\"])\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Import project-specific modules\n",
                "# These modules contain the implementations of the various components of the system,\n",
                "# such as data loading, environment interaction, model architectures, and training algorithms.\n",
                "from src.data.database import DatabaseConnector\n",
                "from src.data.data_loader import JobRecommendationDataset, ReplayBuffer\n",
                "from src.environments.job_env import JobRecommendationEnv\n",
                "from src.models.q_network import QNetwork\n",
                "from src.models.world_model import WorldModel\n",
                "from src.training.agent import DynaQAgent\n",
                "from src.utils.visualizer import Visualizer\n",
                "from src.utils.evaluator import Evaluator\n",
                "\n",
                "print(\"Project modules imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Database Connection\n",
                "\n",
                "The system connects to a MongoDB Atlas database containing job postings and candidate information. The database includes collections for:\n",
                "- Job text data (descriptions, requirements, etc.)\n",
                "- Job embedding vectors (semantic representations)\n",
                "- Candidate profiles\n",
                "- Candidate embedding vectors\n",
                "\n",
                "The connection is established using environment variables for credentials."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get reward strategy settings from config\n",
                "if not STRATEGY_CONFIG[\"hybrid\"][\"enabled\"]:\n",
                "    raise ValueError(\"Hybrid strategy is not enabled in config\")\n",
                "\n",
                "reward_strategy = \"hybrid\"  # Default to hybrid strategy\n",
                "cosine_weight = STRATEGY_CONFIG[\"hybrid\"][\"initial_cosine_weight\"]\n",
                "\n",
                "# Initialize database connector\n",
                "try:\n",
                "    db = DatabaseConnector()  # Uses default connection string and db name from config\n",
                "    print(\"Database connection established successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Database connection error: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Loading\n",
                "\n",
                "We'll now access the database to:\n",
                "1. Retrieve candidate (applicant) embedding data\n",
                "2. Sample a subset of jobs from the database\n",
                "3. Retrieve job embedding vectors\n",
                "\n",
                "These operations use the DatabaseConnector methods to efficiently access the data needed for training and evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Query the database to get a list of candidate IDs\n",
                "try:\n",
                "    # Get the candidates_text collection\n",
                "    collection = db.db[db.collections[\"candidates_text\"]]\n",
                "    \n",
                "    # Query for candidate IDs (limit to 10 for demonstration)\n",
                "    candidate_ids = [doc[\"_id\"] for doc in collection.find({}, {\"_id\": 1}).limit(TRAINING_CONFIG[\"num_candidates\"])]\n",
                "    \n",
                "    if not candidate_ids:\n",
                "        raise ValueError(\"No candidates found in the database\")\n",
                "    \n",
                "    # Select the first candidate for demonstration\n",
                "    target_candidate_id = candidate_ids[0]\n",
                "    print(f\"Selected target candidate ID: {target_candidate_id}\")\n",
                "    \n",
                "    # Get the candidate embedding directly from the database connector\n",
                "    candidate_embedding = db.get_applicant_state(target_candidate_id)\n",
                "    print(f\"Candidate embedding shape: {candidate_embedding.shape}\")\n",
                "    \n",
                "    # Sample jobs from the database with embedding validation to ensure all have required fields\n",
                "    sampled_jobs = db.sample_candidate_jobs(n=TRAINING_CONFIG[\"num_jobs\"], validate_embeddings=True)\n",
                "    job_ids = [job[\"_id\"] for job in sampled_jobs]\n",
                "    print(f\"Sampled {len(job_ids)} jobs from the database (all with valid embeddings)\")\n",
                "    \n",
                "    # Get job vectors for the sampled jobs directly from the database connector\n",
                "    job_vectors = db.get_job_vectors(job_ids)\n",
                "    print(f\"Fetched {len(job_vectors)} job vectors\")\n",
                "    \n",
                "    # Convert job vectors to NumPy array for easier handling\n",
                "    job_vectors_np = np.array([tensor.cpu().numpy() for tensor in job_vectors])\n",
                "    print(f\"Job vectors shape: {job_vectors_np.shape}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error loading data: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Initialization\n",
                "\n",
                "We'll initialize the neural networks used in the Dyna-Q algorithm:\n",
                "\n",
                "1. **Q-Network**: Approximates the value function mapping state-action pairs to expected returns\n",
                "2. **Target Q-Network**: A copy of the Q-Network used for stable learning\n",
                "3. **World Model**: Predicts next states and rewards based on current states and actions\n",
                "\n",
                "Each network is configured according to the parameters specified in the configuration file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Q-Network\n",
                "q_network = QNetwork(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    hidden_dims=MODEL_CONFIG[\"q_network\"][\"hidden_dims\"],\n",
                "    dropout_rate=MODEL_CONFIG[\"q_network\"][\"dropout_rate\"],\n",
                "    activation=MODEL_CONFIG[\"q_network\"][\"activation\"]\n",
                ").to(device)\n",
                "\n",
                "# Initialize World Model\n",
                "world_model = WorldModel(\n",
                "    input_dim=MODEL_CONFIG[\"world_model\"][\"input_dim\"],\n",
                "    hidden_dims=MODEL_CONFIG[\"world_model\"][\"hidden_dims\"],\n",
                "    dropout_rate=MODEL_CONFIG[\"world_model\"][\"dropout_rate\"],\n",
                "    activation=MODEL_CONFIG[\"world_model\"][\"activation\"]\n",
                ").to(device)\n",
                "\n",
                "# Initialize DynaQAgent\n",
                "agent = DynaQAgent(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    q_network=q_network,\n",
                "    world_model=world_model,\n",
                "    training_strategy=reward_strategy,\n",
                "    device=device,\n",
                "    target_applicant_id=target_candidate_id\n",
                ")\n",
                "\n",
                "# Initialize Visualizer\n",
                "visualizer = Visualizer()\n",
                "\n",
                "# Initialize Evaluator\n",
                "evaluator = Evaluator()\n",
                "\n",
                "# Initialize environment with required parameters\n",
                "env = JobRecommendationEnv(\n",
                "    db_connector=db,\n",
                "    reward_strategy=reward_strategy,\n",
                "    random_seed=TRAINING_CONFIG[\"random_seed\"]\n",
                ")\n",
                "\n",
                "# Set the target candidate ID for the environment\n",
                "env.reset(applicant_id=target_candidate_id)\n",
                "\n",
                "# Verify environment initialization\n",
                "if not hasattr(env, 'current_state'):\n",
                "    raise ValueError(\"Environment initialization failed - missing current_state\")\n",
                "\n",
                "print(f\"Environment initialized with {reward_strategy} reward strategy.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training\n",
                "\n",
                "We'll now run the main training loop using the agent's training interface.\n",
                "The agent will:\n",
                "1. Interact with the environment\n",
                "2. Store experiences in its replay buffer\n",
                "3. Update networks using the stored experiences\n",
                "4. Track and report training metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training parameters from config\n",
                "num_episodes = TRAINING_CONFIG[\"num_episodes\"]\n",
                "max_steps_per_episode = TRAINING_CONFIG[\"max_steps_per_episode\"]\n",
                "\n",
                "print(\"Starting main training loop...\")\n",
                "\n",
                "# Run training using agent's interface\n",
                "training_metrics = agent.train(\n",
                "    env=env,\n",
                "    num_episodes=num_episodes,\n",
                "    max_steps_per_episode=max_steps_per_episode,\n",
                "    applicant_ids=[target_candidate_id]\n",
                ")\n",
                "\n",
                "# Plot training metrics\n",
                "visualizer.plot_training_metrics(\n",
                "    metrics={\n",
                "        'q_losses': training_metrics['q_losses'],\n",
                "        'world_losses': training_metrics['world_losses'],\n",
                "        'episode_rewards': training_metrics['episode_rewards']\n",
                "    },\n",
                "    title=\"Training Metrics\"\n",
                ")\n",
                "\n",
                "print(\"Training completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluation\n",
                "\n",
                "Finally, we'll evaluate the trained agent using the Evaluator class.\n",
                "This will:\n",
                "1. Test the agent on a set of evaluation episodes\n",
                "2. Compare performance against baseline\n",
                "3. Generate evaluation metrics and visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation parameters from config\n",
                "num_eval_episodes = EVAL_CONFIG[\"num_episodes\"]\n",
                "baseline_strategy = EVAL_CONFIG[\"baseline_strategy\"]\n",
                "\n",
                "print(\"Starting evaluation...\")\n",
                "\n",
                "# Create a baseline agent for comparison\n",
                "baseline_agent = DynaQAgent(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    training_strategy=baseline_strategy,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Use evaluator's compare_agents method which internally calls evaluate_agent\n",
                "comparison_results = evaluator.compare_agents(\n",
                "    baseline_agent=baseline_agent,\n",
                "    pretrained_agent=agent,\n",
                "    env=env,\n",
                "    applicant_ids=[target_candidate_id],\n",
                "    num_episodes=num_eval_episodes\n",
                ")\n",
                "\n",
                "# Extract the results for visualization\n",
                "evaluation_results = {\n",
                "    'agent_rewards': comparison_results['pretrained']['episode_rewards'],\n",
                "    'baseline_rewards': comparison_results['baseline']['episode_rewards']\n",
                "}\n",
                "\n",
                "# Plot evaluation results\n",
                "visualizer.plot_evaluation_results(\n",
                "    baseline_rewards=evaluation_results['baseline_rewards'],\n",
                "    pretrained_rewards=evaluation_results['agent_rewards'],\n",
                "    title=\"Evaluation Results\"\n",
                ")\n",
                "\n",
                "print(\"Evaluation completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Generate Job Recommendations\n",
                "\n",
                "Now let's use our trained agent to generate personalized job recommendations for our target candidate. We'll:\n",
                "\n",
                "1. Use the trained Q-network to evaluate jobs\n",
                "2. Select the top-K jobs with highest Q-values\n",
                "3. Display the recommendations along with job details\n",
                "\n",
                "These recommendations represent the jobs the agent believes are most suitable for the candidate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define testing parameters\n",
                "num_recommendations = EVAL_CONFIG[\"top_k_recommendations\"]\n",
                "test_epsilon = 0.0  # No exploration during testing (pure exploitation)\n",
                "\n",
                "# Initialize lists to store recommendations\n",
                "recommended_jobs = []\n",
                "recommendation_scores = []\n",
                "\n",
                "# Make a copy of job data for testing\n",
                "test_job_ids = job_ids.copy()\n",
                "test_job_vectors = job_vectors_np.copy()\n",
                "\n",
                "# Reset the environment to get the initial state\n",
                "state = env.reset(applicant_id=target_candidate_id)\n",
                "\n",
                "# Generate top-K recommendations\n",
                "for _ in range(num_recommendations):\n",
                "    # Convert job vectors to tensor format\n",
                "    action_tensors = [torch.from_numpy(vector).float() for vector in test_job_vectors]\n",
                "    \n",
                "    # Select best job according to Q-network\n",
                "    action_idx, _ = agent.select_action(state, action_tensors, eval_mode=True)\n",
                "    \n",
                "    # Get the corresponding job_id and job_vector\n",
                "    job_id = test_job_ids[action_idx]\n",
                "    job_vector = test_job_vectors[action_idx]\n",
                "    \n",
                "    # Calculate Q-value for logging\n",
                "    with torch.no_grad():\n",
                "        q_value = agent.q_network(\n",
                "            torch.tensor(state, device=device).unsqueeze(0),\n",
                "            torch.from_numpy(job_vector).float().to(device).unsqueeze(0)\n",
                "        ).item()\n",
                "    \n",
                "    recommended_jobs.append(job_id)\n",
                "    recommendation_scores.append(q_value)\n",
                "    \n",
                "    # Remove selected job from consideration for next selection\n",
                "    test_job_ids.pop(action_idx)\n",
                "    test_job_vectors = np.delete(test_job_vectors, action_idx, axis=0)\n",
                "\n",
                "# Display recommendations with details\n",
                "print(\"\\n=== Top Job Recommendations ===\\n\")\n",
                "for i, (job_id, score) in enumerate(zip(recommended_jobs, recommendation_scores)):\n",
                "    print(f\"Recommendation #{i+1}: [Q-Value: {score:.4f}]\")\n",
                "    print(f\"Job ID: {job_id}\")\n",
                "    \n",
                "    # Retrieve and display job details\n",
                "    try:\n",
                "        job_details = db.get_job_details(job_id)\n",
                "        print(f\"Title: {job_details.get('job_title', 'N/A')}\")\n",
                "        \n",
                "        # Display truncated description\n",
                "        description = job_details.get('description', 'N/A')\n",
                "        print(f\"Description: {description[:100]}...\" if len(description) > 100 else f\"Description: {description}\")\n",
                "        \n",
                "        # Display technical skills if available\n",
                "        if 'technical_skills' in job_details:\n",
                "            print(f\"Technical Skills: {', '.join(job_details['technical_skills'])}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error retrieving job details: {e}\")\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Conclusion and Next Steps\n",
                "\n",
                "In this notebook, we've implemented and demonstrated a Neural Dyna-Q job recommendation system. This approach combines the strengths of deep reinforcement learning with model-based planning to provide personalized job recommendations.\n",
                "\n",
                "### 10.1 Key Accomplishments\n",
                "\n",
                "1. **Data Integration**: Connected to the MongoDB database to retrieve real candidate and job data\n",
                "2. **Neural Networks**: Implemented deep Q-network and world model for value function and dynamics prediction\n",
                "3. **Dyna-Q Algorithm**: Combined direct RL with model-based planning for efficient learning\n",
                "4. **Personalized Recommendations**: Generated job recommendations tailored to a specific candidate\n",
                "\n",
                "### 10.2 Potential Improvements\n",
                "\n",
                "1. **Extended Training**: Train for more episodes to improve recommendation quality\n",
                "2. **Hyperparameter Tuning**: Optimize learning rates, network architectures, and other parameters\n",
                "3. **Advanced Reward Functions**: Implement more sophisticated reward strategies using LLMs\n",
                "4. **User Feedback**: Incorporate real user feedback to improve recommendations\n",
                "\n",
                "### 10.3 Applications\n",
                "\n",
                "This system could be deployed as:\n",
                "- A personalized job recommendation service for job seekers\n",
                "- A candidate-job matching tool for recruiters\n",
                "- A component in a larger career guidance system"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean up resources\n",
                "db.close()\n",
                "print(\"Database connection closed.\")\n",
                "print(\"Notebook execution complete.\")"
            ]
        }
    ]
}
   