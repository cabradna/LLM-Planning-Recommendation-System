{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10"
        },
        "colab": {
            "provenance": []
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Neural Dyna-Q for Job Recommendation: Model-Based RL Approach\n",
                "\n",
                "**Author:** LLM Lab Team  \n",
                "**Date:** 2023-10-15"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Introduction and Overview\n",
                "\n",
                "This notebook demonstrates a Neural Dyna-Q approach to personalized job recommendation. By combining deep reinforcement learning with model-based planning, we build a system that learns to recommend jobs that match candidate skills and preferences.\n",
                "\n",
                "### 1.1 Core Algorithm Concepts\n",
                "\n",
                "The Neural Dyna-Q algorithm integrates these key components:\n",
                "\n",
                "- **Q-Learning**: Learning a value function that predicts the utility of job recommendations\n",
                "- **World Model**: A neural network that models the environment dynamics\n",
                "- **Planning**: Using the world model to simulate experiences and improve the value function\n",
                "- **Exploration Strategy**: Balancing exploration and exploitation with epsilon-greedy policy\n",
                "\n",
                "### 1.2 Key Features of Implementation\n",
                "\n",
                "- **Single-Applicant Specialization**: Each agent is trained for one specific applicant\n",
                "- **Neural Networks**: Deep learning for value function and environment dynamics approximation\n",
                "- **Multiple Reward Generation Strategies**: Cosine similarity, LLM feedback, and hybrid approaches\n",
                "- **Dyna-Q Algorithm**: Combines direct RL with model-based planning\n",
                "- **Tensor-Based Caching**: Efficient GPU-accelerated data access for faster training\n",
                "\n",
                "### 1.3 Expected Outcomes\n",
                "\n",
                "By the end of this notebook, we'll have:\n",
                "1. A trained job recommendation agent for a specific candidate\n",
                "2. Visualizations of the learning process\n",
                "3. A list of personalized job recommendations\n",
                "4. Insights into the model's performance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Environment Setup\n",
                "\n",
                "First, we'll set up our environment, install necessary packages, and import libraries. If running in Google Colab, we'll clone the repository and install dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if running in Google Colab\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuration settings for the setup process\n",
                "SETUP_PATH_CONFIG = {\n",
                "    \"repo_url\": \"https://github.com/cabradna/LLM-Planning-Recommendation-System.git\",\n",
                "    \"branch\": \"local_tensors\"\n",
                "}\n",
                "\n",
                "SETUP_STRATEGY_CONFIG = {\n",
                "    \"llm\": {\"enabled\": True},\n",
                "    \"hybrid\": {\"enabled\": False}\n",
                "}\n",
                "\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"Running in Google Colab. Cloning repository and installing dependencies...\")\n",
                "\n",
                "    try:\n",
                "        repo_url = SETUP_PATH_CONFIG[\"repo_url\"]\n",
                "        branch = SETUP_PATH_CONFIG[\"branch\"]\n",
                "        repo_name = Path(repo_url).stem\n",
                "\n",
                "        print(f\"Cloning repository: {repo_url} (branch: {branch})\")\n",
                "        subprocess.run([\"git\", \"clone\", \"-q\", repo_url], check=True)\n",
                "        \n",
                "        cloned_repo_path = Path(repo_name)\n",
                "        if cloned_repo_path.is_dir():\n",
                "            os.chdir(cloned_repo_path)\n",
                "            print(f\"Changed directory to: {os.getcwd()}\")\n",
                "            \n",
                "            # Checkout the correct branch\n",
                "            print(f\"Checking out branch: {branch}\")\n",
                "            subprocess.run([\"git\", \"checkout\", branch], check=True)\n",
                "        else:\n",
                "            raise FileNotFoundError(f\"Cloned repository directory '{repo_name}' not found\")\n",
                "\n",
                "        requirements_path = Path(\"requirements.txt\")\n",
                "        if requirements_path.is_file():\n",
                "            print(\"Installing base requirements from requirements.txt...\")\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_path)], check=True)\n",
                "        else:\n",
                "            print(\"Warning: requirements.txt not found. Skipping base requirements installation.\")\n",
                "\n",
                "        print(\"Installing project package in editable mode...\")\n",
                "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"], check=True)\n",
                "\n",
                "        if SETUP_STRATEGY_CONFIG[\"llm\"][\"enabled\"] or SETUP_STRATEGY_CONFIG[\"hybrid\"][\"enabled\"]:\n",
                "            print(\"Installing LLM-related packages (transformers, accelerate, bitsandbytes)...\")\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"accelerate\"], check=True)\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bitsandbytes>=0.45.0\"], check=True)\n",
                "            print(\"LLM-related packages installed successfully.\")\n",
                "\n",
                "        print(\"Repository cloned and dependencies installed successfully.\")\n",
                "\n",
                "        project_root = Path(os.getcwd()).absolute()\n",
                "        if str(project_root) not in sys.path:\n",
                "            sys.path.insert(0, str(project_root))\n",
                "            print(f\"Added {project_root} to sys.path\")\n",
                "\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"Setup command failed: {e}\")\n",
                "        print(\"Please check the repository URL, requirements file, setup.py, and Colab environment.\")\n",
                "        raise\n",
                "    except FileNotFoundError as e:\n",
                "        print(f\"Setup error: {e}\")\n",
                "        raise\n",
                "else:\n",
                "    print(\"Not running in Google Colab. Assuming local environment is set up.\")\n",
                "\n",
                "    try:\n",
                "        project_root = Path(__file__).resolve().parent.parent\n",
                "    except NameError:\n",
                "        project_root = Path('.').absolute()\n",
                "        print(f\"Warning: __file__ not defined. Assuming project root is CWD: {project_root}\")\n",
                "\n",
                "    if str(project_root) not in sys.path:\n",
                "        sys.path.insert(0, str(project_root))\n",
                "        print(f\"Added {project_root} to sys.path for local execution.\")\n",
                "        \n",
                "    # Verify we're on the correct branch locally\n",
                "    try:\n",
                "        result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
                "        current_branch = result.stdout.strip()\n",
                "        if current_branch != SETUP_PATH_CONFIG[\"branch\"]:\n",
                "            print(f\"Warning: Current branch is '{current_branch}', but code expects '{SETUP_PATH_CONFIG['branch']}'\")\n",
                "            print(\"Please switch to the correct branch for this implementation.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not verify git branch: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard imports for data manipulation, visualization, and deep learning\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import random\n",
                "from tqdm.notebook import tqdm\n",
                "import torch\n",
                "from pathlib import Path\n",
                "\n",
                "# Import configuration settings\n",
                "from config.config import (\n",
                "    DB_CONFIG,\n",
                "    MODEL_CONFIG,\n",
                "    TRAINING_CONFIG,\n",
                "    STRATEGY_CONFIG,\n",
                "    PATH_CONFIG,\n",
                "    EVAL_CONFIG,\n",
                "    HF_CONFIG,\n",
                "    ENV_CONFIG\n",
                ")\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "random.seed(ENV_CONFIG[\"random_seed\"])\n",
                "np.random.seed(ENV_CONFIG[\"random_seed\"])\n",
                "torch.manual_seed(ENV_CONFIG[\"random_seed\"])\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(ENV_CONFIG[\"random_seed\"])\n",
                "\n",
                "# Determine the device to use for PyTorch computations\n",
                "device = torch.device(TRAINING_CONFIG[\"device\"])\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Import project-specific modules\n",
                "from src.data.database import DatabaseConnector\n",
                "from src.data.data_loader import JobRecommendationDataset, ReplayBuffer\n",
                "from src.environments.job_env import JobRecommendationEnv, LLMSimulatorEnv, HybridEnv\n",
                "from src.models.q_network import QNetwork\n",
                "from src.models.world_model import WorldModel\n",
                "from src.training.agent import DynaQAgent\n",
                "from src.utils.visualizer import Visualizer\n",
                "from src.utils.evaluator import Evaluator\n",
                "from src.data.tensor_cache import TensorCache\n",
                "\n",
                "print(\"Project modules imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Database Connection\n",
                "\n",
                "The system connects to a MongoDB Atlas database containing job postings and candidate information. The database includes collections for:\n",
                "- Job text data (descriptions, requirements, etc.)\n",
                "- Job embedding vectors (semantic representations)\n",
                "- Candidate profiles\n",
                "- Candidate embedding vectors\n",
                "\n",
                "The connection is established using environment variables for credentials."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get reward strategy settings from config\n",
                "if not STRATEGY_CONFIG[\"hybrid\"][\"enabled\"]:\n",
                "    raise ValueError(\"Hybrid strategy is not enabled in config\")\n",
                "\n",
                "reward_strategy = \"hybrid\"  # Default to hybrid strategy\n",
                "cosine_weight = STRATEGY_CONFIG[\"hybrid\"][\"initial_cosine_weight\"]\n",
                "\n",
                "# Initialize database connector\n",
                "try:\n",
                "    db_connector = DatabaseConnector()\n",
                "    print(\"Database connection established successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Database connection error: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Loading and Tensor Cache Initialization\n",
                "\n",
                "We'll now access the database to:\n",
                "1. Retrieve candidate (applicant) embedding data\n",
                "2. Initialize the tensor cache for efficient data access\n",
                "3. Load all valid jobs into GPU memory\n",
                "4. Set up the environment with the tensor cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Query the database to get a list of candidate IDs\n",
                "try:\n",
                "    collection = db_connector.db[db_connector.collections[\"candidates_text\"]]\n",
                "    candidate_ids = [doc[\"_id\"] for doc in collection.find({}, {\"_id\": 1}).limit(TRAINING_CONFIG[\"num_candidates\"])]\n",
                "    \n",
                "    if not candidate_ids:\n",
                "        raise ValueError(\"No candidates found in the database\")\n",
                "    \n",
                "    target_candidate_id = candidate_ids[0]\n",
                "    print(f\"Selected target candidate ID: {target_candidate_id}\")\n",
                "    \n",
                "    # Get the candidate embedding\n",
                "    candidate_embedding = db_connector.get_applicant_state(target_candidate_id)\n",
                "    print(f\"Candidate embedding shape: {candidate_embedding.shape}\")\n",
                "    \n",
                "    # Initialize tensor cache\n",
                "    print(f\"Initializing tensor cache on {TRAINING_CONFIG['device']}...\")\n",
                "    \n",
                "    tensor_cache = TensorCache(device=TRAINING_CONFIG[\"device\"])\n",
                "    \n",
                "    # Load all data from database to cache\n",
                "    tensor_cache.copy_from_database(\n",
                "        db_connector=db_connector,\n",
                "        applicant_ids=[target_candidate_id]\n",
                "    )\n",
                "    \n",
                "    # Print cache statistics\n",
                "    stats = tensor_cache.cache_stats()\n",
                "    print(f\"Cache initialized with {stats['job_count']} jobs\")\n",
                "    print(f\"Initialization time: {stats['initialization_time']:.2f} seconds\")\n",
                "    print(f\"Memory device: {stats['device']}\")\n",
                "    \n",
                "    # Create environment with cache\n",
                "    if reward_strategy == \"cosine\":\n",
                "        env = JobRecommendationEnv(\n",
                "            db_connector=db_connector,\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_strategy=\"cosine\",\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    elif reward_strategy == \"llm\":\n",
                "        env = LLMSimulatorEnv(\n",
                "            db_connector=db_connector,\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"],\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    elif reward_strategy == \"hybrid\":\n",
                "        env = HybridEnv(\n",
                "            db_connector=db_connector,\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"],\n",
                "            cosine_weight=STRATEGY_CONFIG[\"hybrid\"][\"cosine_weight_start\"],\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown reward strategy: {reward_strategy}\")\n",
                "    \n",
                "    print(f\"Created {reward_strategy} environment with tensor cache\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error loading data: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Initialization\n",
                "\n",
                "We'll initialize the neural networks used in the Dyna-Q algorithm:\n",
                "\n",
                "1. **Q-Network**: Approximates the value function mapping state-action pairs to expected returns\n",
                "2. **Target Q-Network**: A copy of the Q-Network used for stable learning\n",
                "3. **World Model**: Predicts next states and rewards based on current states and actions\n",
                "\n",
                "Each network is configured according to the parameters specified in the configuration file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Q-Network\n",
                "q_network = QNetwork(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    hidden_dims=MODEL_CONFIG[\"q_network\"][\"hidden_dims\"],\n",
                "    dropout_rate=MODEL_CONFIG[\"q_network\"][\"dropout_rate\"],\n",
                "    activation=MODEL_CONFIG[\"q_network\"][\"activation\"]\n",
                ").to(device)\n",
                "\n",
                "# Initialize World Model\n",
                "world_model = WorldModel(\n",
                "    input_dim=MODEL_CONFIG[\"world_model\"][\"input_dim\"],\n",
                "    hidden_dims=MODEL_CONFIG[\"world_model\"][\"hidden_dims\"],\n",
                "    dropout_rate=MODEL_CONFIG[\"world_model\"][\"dropout_rate\"],\n",
                "    activation=MODEL_CONFIG[\"world_model\"][\"activation\"]\n",
                ").to(device)\n",
                "\n",
                "# Initialize DynaQAgent\n",
                "agent = DynaQAgent(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    q_network=q_network,\n",
                "    world_model=world_model,\n",
                "    training_strategy=reward_strategy,\n",
                "    device=device,\n",
                "    target_applicant_id=target_candidate_id\n",
                ")\n",
                "\n",
                "# Initialize Visualizer\n",
                "visualizer = Visualizer()\n",
                "\n",
                "# Initialize Evaluator\n",
                "evaluator = Evaluator()\n",
                "\n",
                "# Initialize environment with required parameters\n",
                "env = JobRecommendationEnv(\n",
                "    db_connector=db_connector,\n",
                "    reward_strategy=reward_strategy,\n",
                "    random_seed=TRAINING_CONFIG[\"random_seed\"]\n",
                ")\n",
                "\n",
                "# Set the target candidate ID for the environment\n",
                "env.reset(applicant_id=target_candidate_id)\n",
                "\n",
                "# Verify environment initialization\n",
                "if not hasattr(env, 'current_state'):\n",
                "    raise ValueError(\"Environment initialization failed - missing current_state\")\n",
                "\n",
                "print(f\"Environment initialized with {reward_strategy} reward strategy.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. LLM Integration (If Using LLM-Based Reward Strategy)\n",
                "\n",
                "If we're using an LLM-based reward strategy (either \"llm\" or \"hybrid\"), we need to set up a language model to simulate candidate responses to job recommendations. This helps in generating more realistic rewards based on semantic understanding of both candidate profiles and job descriptions.\n",
                "\n",
                "Note: This section will only execute if we're using an LLM-based strategy and running in a Colab environment with sufficient resources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if STRATEGY_CONFIG[\"llm\"][\"enabled\"] and IN_COLAB:\n",
                "    try:\n",
                "        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "        from huggingface_hub import login\n",
                "        \n",
                "        # Authenticate with Hugging Face using token from file\n",
                "        try:\n",
                "            # Get token path from config\n",
                "            token_path = HF_CONFIG[\"token_path\"]\n",
                "            \n",
                "            # Read the token from file\n",
                "            with open(token_path, \"r\") as f:\n",
                "                token = f.read().strip()\n",
                "                \n",
                "            # Login to Hugging Face\n",
                "            login(token=token)\n",
                "            print(f\"Successfully logged in to Hugging Face using token from {token_path}\")\n",
                "        except Exception as e:\n",
                "            print(f\"Error authenticating with Hugging Face: {e}\")\n",
                "            print(\"Will attempt to load model without authentication\")\n",
                "        \n",
                "        # LLM configuration from config\n",
                "        model_id = STRATEGY_CONFIG[\"llm\"][\"model_id\"]\n",
                "        \n",
                "        # Configure quantization settings from config\n",
                "        bnb_config = BitsAndBytesConfig(\n",
                "            load_in_4bit=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"load_in_4bit\"],\n",
                "            bnb_4bit_quant_type=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"quant_type\"],\n",
                "            bnb_4bit_use_nested_quant=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"use_nested_quant\"],\n",
                "            bnb_4bit_compute_dtype=torch.bfloat16 \n",
                "        )\n",
                "        \n",
                "        print(f\"Loading tokenizer for model: {model_id}\")\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "        \n",
                "        print(f\"Loading model {model_id} with 4-bit quantization...\")\n",
                "        \n",
                "        # Load the model with quantization for efficiency\n",
                "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_id,\n",
                "            quantization_config=bnb_config,\n",
                "            device_map=\"auto\"\n",
                "        )\n",
                "        \n",
                "        print(f\"LLM model loaded successfully.\")\n",
                "        \n",
                "        # Set up LLM in the environment for reward calculation and capture returned environment\n",
                "        # setup_llm may return a new environment instance if current one doesn't support LLM\n",
                "        env = env.setup_llm(llm_model, tokenizer)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading LLM: {e}\")\n",
                "        print(\"Switching to cosine similarity reward strategy.\")\n",
                "        env.reward_strategy = \"cosine\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Training Setup\n",
                "\n",
                "We'll now set up the training process using the DynaQAgent's built-in training interface.\n",
                "The agent handles:\n",
                "1. Experience replay buffer\n",
                "2. Q-Network and World Model updates\n",
                "3. Loss tracking and metrics\n",
                "4. Model checkpointing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the DynaQAgent with all required components\n",
                "agent = DynaQAgent(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    q_network=q_network,\n",
                "    world_model=world_model,\n",
                "    training_strategy=reward_strategy,\n",
                "    device=device,\n",
                "    target_applicant_id=target_candidate_id\n",
                ")\n",
                "\n",
                "print(\"Dyna-Q agent initialized successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Pretraining Phase\n",
                "\n",
                "In the pretraining phase, we'll use the agent's built-in pretraining method to:\n",
                "1. Generate initial experiences\n",
                "2. Train the Q-network and world model\n",
                "3. Track training metrics\n",
                "4. Save model checkpoints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define pretraining parameters from config\n",
                "num_pretraining_samples = min(TRAINING_CONFIG[\"pretraining\"][\"num_samples\"], len(job_ids))\n",
                "num_pretraining_epochs = TRAINING_CONFIG[\"pretraining\"][\"num_epochs\"]\n",
                "batch_size = TRAINING_CONFIG[\"batch_size\"]\n",
                "\n",
                "print(f\"Starting pretraining with {reward_strategy} strategy...\")\n",
                "\n",
                "# Generate pretraining data\n",
                "pretraining_data = []\n",
                "state = env.reset(applicant_id=target_candidate_id)\n",
                "\n",
                "for i in tqdm(range(num_pretraining_samples), desc=\"Generating pretraining data\"):\n",
                "    job_idx = i % len(job_ids)\n",
                "    job_id = job_ids[job_idx]\n",
                "    job_vector = job_vectors_np[job_idx]\n",
                "    \n",
                "    next_state, reward, done, _ = env.step(job_idx)\n",
                "    pretraining_data.append((state, job_vector, reward, next_state))\n",
                "    state = next_state\n",
                "\n",
                "# Convert to numpy arrays\n",
                "states = np.array([d[0] for d in pretraining_data])\n",
                "actions = np.array([d[1] for d in pretraining_data])\n",
                "rewards = np.array([d[2] for d in pretraining_data])\n",
                "next_states = np.array([d[3] for d in pretraining_data])\n",
                "\n",
                "# Verify dimensions\n",
                "print(f\"States shape: {states.shape}\")\n",
                "print(f\"Actions shape: {actions.shape}\")\n",
                "print(f\"Expected state_dim: {MODEL_CONFIG['q_network']['state_dim']}\")\n",
                "print(f\"Expected action_dim: {MODEL_CONFIG['q_network']['action_dim']}\")\n",
                "\n",
                "# Ensure states and actions have correct dimensions\n",
                "if states.shape[1] != MODEL_CONFIG['q_network']['state_dim']:\n",
                "    raise ValueError(f\"State dimension mismatch. Got {states.shape[1]}, expected {MODEL_CONFIG['q_network']['state_dim']}\")\n",
                "if actions.shape[1] != MODEL_CONFIG['q_network']['action_dim']:\n",
                "    raise ValueError(f\"Action dimension mismatch. Got {actions.shape[1]}, expected {MODEL_CONFIG['q_network']['action_dim']}\")\n",
                "\n",
                "# Run pretraining using agent's built-in pretrain method\n",
                "pretraining_metrics = agent.pretrain(\n",
                "    states=states,\n",
                "    actions=actions,\n",
                "    rewards=rewards,\n",
                "    next_states=next_states,\n",
                "    num_epochs=num_pretraining_epochs,\n",
                "    batch_size=batch_size\n",
                ")\n",
                "\n",
                "# Plot pretraining metrics\n",
                "visualizer.plot_training_metrics(\n",
                "    metrics={\n",
                "        'q_losses': pretraining_metrics['q_losses'],\n",
                "        'world_losses': pretraining_metrics['world_losses']\n",
                "    },\n",
                "    title=\"Pretraining Losses\"\n",
                ")\n",
                "\n",
                "print(\"Pretraining completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Main Training Loop\n",
                "\n",
                "Now we'll run the main training loop using the agent's training interface.\n",
                "The agent will:\n",
                "1. Interact with the environment\n",
                "2. Store experiences in its replay buffer\n",
                "3. Update networks using the stored experiences\n",
                "4. Track and report training metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training parameters from config\n",
                "num_episodes = TRAINING_CONFIG[\"num_episodes\"]\n",
                "max_steps_per_episode = TRAINING_CONFIG[\"max_steps_per_episode\"]\n",
                "\n",
                "print(\"Starting main training loop...\")\n",
                "\n",
                "# Set up for multiple experiments\n",
                "num_experiments = 5  # Number of experiments to run\n",
                "experiment_results = {\n",
                "    'q_network_loss': [],\n",
                "    'world_model_loss': [],\n",
                "    'episode_reward': [],\n",
                "    'eval_reward': []\n",
                "}\n",
                "\n",
                "# Create directory for saving results\n",
                "results_dir = os.path.join(PATH_CONFIG[\"results_dir\"], \"multi_experiment\")\n",
                "os.makedirs(results_dir, exist_ok=True)\n",
                "\n",
                "# Run multiple experiments\n",
                "for exp_idx in range(num_experiments):\n",
                "    print(f\"\\nStarting experiment {exp_idx+1}/{num_experiments}\")\n",
                "    \n",
                "    # Reinitialize agent for each experiment to ensure independence\n",
                "    if exp_idx > 0:\n",
                "        q_network = QNetwork(\n",
                "            state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "            action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "            hidden_dims=MODEL_CONFIG[\"q_network\"][\"hidden_dims\"],\n",
                "            dropout_rate=MODEL_CONFIG[\"q_network\"][\"dropout_rate\"],\n",
                "            activation=MODEL_CONFIG[\"q_network\"][\"activation\"]\n",
                "        ).to(device)\n",
                "\n",
                "        world_model = WorldModel(\n",
                "            input_dim=MODEL_CONFIG[\"world_model\"][\"input_dim\"],\n",
                "            hidden_dims=MODEL_CONFIG[\"world_model\"][\"hidden_dims\"],\n",
                "            dropout_rate=MODEL_CONFIG[\"world_model\"][\"dropout_rate\"],\n",
                "            activation=MODEL_CONFIG[\"world_model\"][\"activation\"]\n",
                "        ).to(device)\n",
                "        \n",
                "        agent = DynaQAgent(\n",
                "            state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "            action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "            q_network=q_network,\n",
                "            world_model=world_model,\n",
                "            training_strategy=reward_strategy,\n",
                "            device=device,\n",
                "            target_applicant_id=target_candidate_id\n",
                "        )\n",
                "    \n",
                "    # Run training for this experiment\n",
                "    training_metrics = agent.train(\n",
                "        env=env,\n",
                "        num_episodes=num_episodes,\n",
                "        max_steps_per_episode=max_steps_per_episode,\n",
                "        applicant_ids=[target_candidate_id]\n",
                "    )\n",
                "    \n",
                "    # Store results for this experiment\n",
                "    for key in experiment_results:\n",
                "        if key in training_metrics:\n",
                "            experiment_results[key].append(training_metrics[key])\n",
                "    \n",
                "    # Save individual experiment results\n",
                "    exp_file = os.path.join(results_dir, f\"experiment_{exp_idx+1}.pt\")\n",
                "    torch.save(training_metrics, exp_file)\n",
                "    print(f\"Saved experiment {exp_idx+1} results to {exp_file}\")\n",
                "\n",
                "# Save aggregated results\n",
                "aggregated_file = os.path.join(results_dir, \"aggregated_results.pt\")\n",
                "torch.save(experiment_results, aggregated_file)\n",
                "print(f\"Saved aggregated results to {aggregated_file}\")\n",
                "\n",
                "# Visualize results using the Visualizer\n",
                "visualizer.plot_experiment_results(\n",
                "    experiment_results=experiment_results,\n",
                "    title_prefix=\"Dyna-Q Performance\",\n",
                "    filename_prefix=\"multi_experiment\"\n",
                ")\n",
                "\n",
                "# Plot training metrics for the last experiment\n",
                "visualizer.plot_training_metrics(\n",
                "    metrics={\n",
                "        'q_losses': training_metrics['q_network_loss'],\n",
                "        'world_losses': training_metrics['world_model_loss'],\n",
                "        'episode_rewards': training_metrics['episode_reward']\n",
                "    },\n",
                "    title=\"Training Metrics (Last Experiment)\"\n",
                ")\n",
                "\n",
                "print(\"Multiple experiments completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10.1 Loading and Visualizing Saved Experiment Results\n",
                "\n",
                "This section provides functionality to load and analyze results from previously run experiments.\n",
                "This is useful for revisiting experiment results without having to rerun the experiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_experiment_results(results_dir):\n",
                "    \"\"\"\n",
                "    Load saved experiment results for visualization.\n",
                "    \n",
                "    Args:\n",
                "        results_dir: Directory containing experiment results\n",
                "    \n",
                "    Returns:\n",
                "        dict: Loaded experiment results\n",
                "    \"\"\"\n",
                "    # Check if aggregated results exist\n",
                "    aggregated_file = os.path.join(results_dir, \"aggregated_results.pt\")\n",
                "    \n",
                "    if os.path.exists(aggregated_file):\n",
                "        print(f\"Loading aggregated results from {aggregated_file}\")\n",
                "        return torch.load(aggregated_file)\n",
                "    else:\n",
                "        # Try to load individual experiment files\n",
                "        import glob\n",
                "        exp_files = glob.glob(os.path.join(results_dir, \"experiment_*.pt\"))\n",
                "        \n",
                "        if not exp_files:\n",
                "            print(f\"No experiment files found in {results_dir}\")\n",
                "            return {}\n",
                "        \n",
                "        print(f\"Found {len(exp_files)} individual experiment files\")\n",
                "        \n",
                "        # Load individual files and combine them\n",
                "        combined_results = {\n",
                "            'q_network_loss': [],\n",
                "            'world_model_loss': [],\n",
                "            'episode_reward': [],\n",
                "            'eval_reward': []\n",
                "        }\n",
                "        \n",
                "        for file in exp_files:\n",
                "            exp_data = torch.load(file)\n",
                "            for key in combined_results:\n",
                "                if key in exp_data:\n",
                "                    combined_results[key].append(exp_data[key])\n",
                "        \n",
                "        return combined_results\n",
                "\n",
                "# Example usage (commented out by default)\n",
                "# results_dir = os.path.join(PATH_CONFIG[\"results_dir\"], \"multi_experiment\")\n",
                "# loaded_results = load_experiment_results(results_dir)\n",
                "# if loaded_results:\n",
                "#     visualizer.plot_experiment_results(\n",
                "#         experiment_results=loaded_results,\n",
                "#         title_prefix=\"Loaded Results\",\n",
                "#         filename_prefix=\"loaded_experiment\"\n",
                "#     )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Evaluation\n",
                "\n",
                "Finally, we'll evaluate the trained agent using the Evaluator class.\n",
                "This will:\n",
                "1. Test the agent on a set of evaluation episodes\n",
                "2. Compare performance against baseline\n",
                "3. Generate evaluation metrics and visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation parameters from config\n",
                "num_eval_episodes = EVAL_CONFIG[\"num_episodes\"]\n",
                "baseline_strategy = EVAL_CONFIG[\"baseline_strategy\"]\n",
                "\n",
                "print(\"Starting evaluation...\")\n",
                "\n",
                "# Create a baseline agent for comparison\n",
                "baseline_agent = DynaQAgent(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    training_strategy=baseline_strategy,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Use evaluator's compare_agents method which internally calls evaluate_agent\n",
                "comparison_results = evaluator.compare_agents(\n",
                "    baseline_agent=baseline_agent,\n",
                "    pretrained_agent=agent,\n",
                "    env=env,\n",
                "    applicant_ids=[target_candidate_id],\n",
                "    num_episodes=num_eval_episodes\n",
                ")\n",
                "\n",
                "# Extract the results for visualization\n",
                "evaluation_results = {\n",
                "    'agent_rewards': comparison_results['pretrained']['episode_rewards'],\n",
                "    'baseline_rewards': comparison_results['baseline']['episode_rewards']\n",
                "}\n",
                "\n",
                "# Plot evaluation results\n",
                "visualizer.plot_evaluation_results(\n",
                "    baseline_rewards=evaluation_results['baseline_rewards'],\n",
                "    pretrained_rewards=evaluation_results['agent_rewards'],\n",
                "    title=\"Evaluation Results\"\n",
                ")\n",
                "\n",
                "print(\"Evaluation completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Generate Job Recommendations\n",
                "\n",
                "Now let's use our trained agent to generate personalized job recommendations for our target candidate. We'll:\n",
                "\n",
                "1. Use the trained Q-network to evaluate jobs\n",
                "2. Select the top-K jobs with highest Q-values\n",
                "3. Display the recommendations along with job details\n",
                "\n",
                "These recommendations represent the jobs the agent believes are most suitable for the candidate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define testing parameters\n",
                "num_recommendations = EVAL_CONFIG[\"top_k_recommendations\"]\n",
                "test_epsilon = 0.0  # No exploration during testing (pure exploitation)\n",
                "\n",
                "# Initialize lists to store recommendations\n",
                "recommended_jobs = []\n",
                "recommendation_scores = []\n",
                "\n",
                "# Make a copy of job data for testing\n",
                "test_job_ids = job_ids.copy()\n",
                "test_job_vectors = job_vectors_np.copy()\n",
                "\n",
                "# Reset the environment to get the initial state\n",
                "state = env.reset(applicant_id=target_candidate_id)\n",
                "\n",
                "# Generate top-K recommendations\n",
                "for _ in range(num_recommendations):\n",
                "    # Convert job vectors to tensor format\n",
                "    action_tensors = [torch.from_numpy(vector).float() for vector in test_job_vectors]\n",
                "    \n",
                "    # Select best job according to Q-network\n",
                "    action_idx, _ = agent.select_action(state, action_tensors, eval_mode=True)\n",
                "    \n",
                "    # Get the corresponding job_id and job_vector\n",
                "    job_id = test_job_ids[action_idx]\n",
                "    job_vector = test_job_vectors[action_idx]\n",
                "    \n",
                "    # Calculate Q-value for logging\n",
                "    with torch.no_grad():\n",
                "        q_value = agent.q_network(\n",
                "            torch.tensor(state, device=device).unsqueeze(0),\n",
                "            torch.from_numpy(job_vector).float().to(device).unsqueeze(0)\n",
                "        ).item()\n",
                "    \n",
                "    recommended_jobs.append(job_id)\n",
                "    recommendation_scores.append(q_value)\n",
                "    \n",
                "    # Remove selected job from consideration for next selection\n",
                "    test_job_ids.pop(action_idx)\n",
                "    test_job_vectors = np.delete(test_job_vectors, action_idx, axis=0)\n",
                "\n",
                "# Display recommendations with details\n",
                "print(\"\\n=== Top Job Recommendations ===\\n\")\n",
                "for i, (job_id, score) in enumerate(zip(recommended_jobs, recommendation_scores)):\n",
                "    print(f\"Recommendation #{i+1}: [Q-Value: {score:.4f}]\")\n",
                "    print(f\"Job ID: {job_id}\")\n",
                "    \n",
                "    # Retrieve and display job details\n",
                "    try:\n",
                "        job_details = db.get_job_details(job_id)\n",
                "        print(f\"Title: {job_details.get('job_title', 'N/A')}\")\n",
                "        \n",
                "        # Display truncated description\n",
                "        description = job_details.get('description', 'N/A')\n",
                "        print(f\"Description: {description[:100]}...\" if len(description) > 100 else f\"Description: {description}\")\n",
                "        \n",
                "        # Display technical skills if available\n",
                "        if 'technical_skills' in job_details:\n",
                "            print(f\"Technical Skills: {', '.join(job_details['technical_skills'])}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error retrieving job details: {e}\")\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Conclusion and Next Steps\n",
                "\n",
                "In this notebook, we've implemented and demonstrated a Neural Dyna-Q job recommendation system. This approach combines the strengths of deep reinforcement learning with model-based planning to provide personalized job recommendations.\n",
                "\n",
                "### 13.1 Key Accomplishments\n",
                "\n",
                "1. **Data Integration**: Connected to the MongoDB database to retrieve real candidate and job data\n",
                "2. **Neural Networks**: Implemented deep Q-network and world model for value function and dynamics prediction\n",
                "3. **Dyna-Q Algorithm**: Combined direct RL with model-based planning for efficient learning\n",
                "4. **Personalized Recommendations**: Generated job recommendations tailored to a specific candidate\n",
                "\n",
                "### 13.2 Potential Improvements\n",
                "\n",
                "1. **Extended Training**: Train for more episodes to improve recommendation quality\n",
                "2. **Hyperparameter Tuning**: Optimize learning rates, network architectures, and other parameters\n",
                "3. **Advanced Reward Functions**: Implement more sophisticated reward strategies using LLMs\n",
                "4. **User Feedback**: Incorporate real user feedback to improve recommendations\n",
                "\n",
                "### 13.3 Applications\n",
                "\n",
                "This system could be deployed as:\n",
                "- A personalized job recommendation service for job seekers\n",
                "- A candidate-job matching tool for recruiters\n",
                "- A component in a larger career guidance system"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean up resources\n",
                "db.close()\n",
                "print(\"Database connection closed.\")\n",
                "print(\"Notebook execution complete.\")"
            ]
        }
    ]
}
   