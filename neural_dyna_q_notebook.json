{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "colab": {
            "provenance": [],
            "include_colab_link": true
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro-markdown"
            },
            "source": [
                "# Neural Dyna-Q for Job Recommendation: Model-Based RL Approach\n",
                "\n",
                "**Author:** LLM Lab Team  \n",
                "**Date:** 2023-10-15"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "overview-markdown"
            },
            "source": [
                "## 1. Introduction and Overview\n",
                "\n",
                "This notebook demonstrates a Neural Dyna-Q approach to personalized job recommendation. By combining deep reinforcement learning with model-based planning, we build a system that learns to recommend jobs that match candidate skills and preferences.\n",
                "\n",
                "### 1.1 Core Algorithm Concepts\n",
                "\n",
                "The Neural Dyna-Q algorithm integrates these key components:\n",
                "\n",
                "- **Q-Learning**: Learning a value function that predicts the utility of job recommendations\n",
                "- **World Model**: A neural network that models the environment dynamics\n",
                "- **Planning**: Using the world model to simulate experiences and improve the value function\n",
                "- **Exploration Strategy**: Balancing exploration and exploitation with epsilon-greedy policy\n",
                "\n",
                "### 1.2 Key Features of Implementation\n",
                "\n",
                "- **Single-Applicant Specialization**: Each agent is trained for one specific applicant\n",
                "- **Neural Networks**: Deep learning for value function and environment dynamics approximation\n",
                "- **Multiple Reward Generation Strategies**: Cosine similarity, LLM feedback, and hybrid approaches\n",
                "- **Dyna-Q Algorithm**: Combines direct RL with model-based planning\n",
                "- **Tensor-Based Caching**: Efficient GPU-accelerated data access for faster training\n",
                "\n",
                "### 1.3 Expected Outcomes\n",
                "\n",
                "By the end of this notebook, we'll have:\n",
                "1. A trained job recommendation agent for a specific candidate\n",
                "2. Visualizations of the learning process\n",
                "3. A list of personalized job recommendations\n",
                "4. Insights into the model's performance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup-markdown"
            },
            "source": [
                "## 2. Environment Setup\n",
                "\n",
                "First, we'll set up our environment, install necessary packages, and import libraries. If running in Google Colab, we'll clone the repository and install dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup-code"
            },
            "outputs": [],
            "source": [
                "# Check if running in Google Colab\n",
                "# Import necessary libraries for setup\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuration settings for the setup process, particularly for Colab environments.\n",
                "SETUP_PATH_CONFIG = {\n",
                "    \"repo_url\": \"https://github.com/cabradna/LLM-Planning-Recommendation-System.git\",  # URL of the repository to clone.\n",
                "    \"branch\": \"local_tensors\"  # Specific branch to use for tensor-based implementation\n",
                "}\n",
                "\n",
                "# Configuration for enabling specific strategies during setup (e.g., LLM, hybrid).\n",
                "SETUP_STRATEGY_CONFIG = {\n",
                "    \"llm\": {\"enabled\": True},  # Enable LLM-related dependency installation.\n",
                "    \"hybrid\": {\"enabled\": False} # Enable hybrid strategy dependency installation.\n",
                "}\n",
                "\n",
                "# Determine if the code is running in Google Colab.\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"Running in Google Colab. Cloning repository and installing dependencies...\")\n",
                "\n",
                "    try:\n",
                "        # Retrieve repository URL from configuration.\n",
                "        repo_url = SETUP_PATH_CONFIG[\"repo_url\"]\n",
                "        repo_name = Path(repo_url).stem  # Extract repository name from the URL.\n",
                "\n",
                "        # Clone the repository.\n",
                "        print(f\"Cloning repository: {repo_url}\")\n",
                "        subprocess.run([\"git\", \"clone\", \"-q\", repo_url], check=True)  # Clone quietly and raise an error if cloning fails.\n",
                "\n",
                "        # Navigate into the cloned repository directory.\n",
                "        cloned_repo_path = Path(repo_name)\n",
                "        if cloned_repo_path.is_dir():\n",
                "            os.chdir(cloned_repo_path)\n",
                "            print(f\"Changed directory to: {os.getcwd()}\")\n",
                "            \n",
                "            # Checkout the specific branch\n",
                "            branch = SETUP_PATH_CONFIG[\"branch\"]\n",
                "            print(f\"Checking out branch: {branch}\")\n",
                "            subprocess.run([\"git\", \"checkout\", branch], check=True)\n",
                "        else:\n",
                "            raise FileNotFoundError(f\"Cloned repository directory '{repo_name}' not found or is not a directory.\")\n",
                "\n",
                "        # Install base requirements from requirements.txt.\n",
                "        requirements_path = Path(\"requirements.txt\")\n",
                "        if requirements_path.is_file():\n",
                "            print(\"Installing base requirements from requirements.txt...\")\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_path)], check=True)  # Use the correct pip and install quietly.\n",
                "        else:\n",
                "            print(\"Warning: requirements.txt not found. Skipping base requirements installation.\")\n",
                "\n",
                "        # Install the project package in editable mode.\n",
                "        print(\"Attempting to install project package in editable mode...\")\n",
                "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"], check=True)  # Install the current package in editable mode.\n",
                "\n",
                "        # Install extra packages if LLM or hybrid strategy is enabled.\n",
                "        if SETUP_STRATEGY_CONFIG[\"llm\"][\"enabled\"] or SETUP_STRATEGY_CONFIG[\"hybrid\"][\"enabled\"]:\n",
                "            print(\"Installing LLM-related packages (transformers, accelerate, bitsandbytes)...\")\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"accelerate\"], check=True)\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bitsandbytes>=0.45.0\"], check=True)\n",
                "            print(\"LLM-related packages installed successfully.\")\n",
                "\n",
                "        print(\"Repository cloned and dependencies installed successfully.\")\n",
                "\n",
                "        # Add the project root to the Python path.\n",
                "        project_root = Path(os.getcwd()).absolute()\n",
                "        if str(project_root) not in sys.path:\n",
                "            sys.path.insert(0, str(project_root))  # Insert at the beginning to prioritize project modules.\n",
                "            print(f\"Added {project_root} to sys.path\")\n",
                "\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"Setup command failed: {e}\")\n",
                "        print(\"Please check the repository URL, requirements file, setup.py, and Colab environment.\")\n",
                "        raise  # Re-raise the exception to halt execution.\n",
                "    except FileNotFoundError as e:\n",
                "        print(f\"Setup error: {e}\")\n",
                "        raise  # Re-raise the exception to halt execution.\n",
                "else:\n",
                "    print(\"Not running in Google Colab. Assuming local environment is set up.\")\n",
                "\n",
                "    try:\n",
                "        # Determine the project root directory.\n",
                "        project_root = Path(__file__).resolve().parent.parent\n",
                "    except NameError:\n",
                "        # Handle cases where __file__ is not defined (e.g., interactive environments).\n",
                "        project_root = Path('.').absolute()\n",
                "        print(f\"Warning: __file__ not defined. Assuming project root is CWD: {project_root}\")\n",
                "\n",
                "    # Add the project root to the Python path if it's not already there.\n",
                "    if str(project_root) not in sys.path:\n",
                "        sys.path.insert(0, str(project_root))\n",
                "        print(f\"Added {project_root} to sys.path for local execution.\")\n",
                "        \n",
                "    # Verify we're on the correct branch\n",
                "    try:\n",
                "        result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
                "        current_branch = result.stdout.strip()\n",
                "        if current_branch != SETUP_PATH_CONFIG[\"branch\"]:\n",
                "            print(f\"Warning: Current branch is '{current_branch}', but code expects '{SETUP_PATH_CONFIG['branch']}'\")\n",
                "            print(\"Please switch to the correct branch for this implementation.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not verify git branch: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports-code"
            },
            "outputs": [],
            "source": [
                "# Standard imports for data manipulation, visualization, and deep learning\n",
                "# Standard imports for data manipulation, visualization, and deep learning\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import random\n",
                "from tqdm.notebook import tqdm  # Use tqdm.notebook for Colab/notebook progress bars\n",
                "import torch\n",
                "from pathlib import Path\n",
                "\n",
                "# Dynamically add the project root to the Python path\n",
                "# This allows importing modules from the project's source code, regardless of the current working directory.\n",
                "try:\n",
                "    # Determine the project root directory based on the location of this script.\n",
                "    # Assumes the script is located in a subdirectory of the project root.\n",
                "    project_root = Path(__file__).resolve().parent.parent\n",
                "except NameError:\n",
                "    # Handle cases where __file__ is not defined, such as in interactive environments.\n",
                "    project_root = Path('.').absolute()\n",
                "\n",
                "# Add the project root to the Python path if it's not already there.\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))  # Prioritize project modules over installed ones\n",
                "\n",
                "# Import configuration settings from the project's config module\n",
                "# These settings define various aspects of the experiment, such as database connections,\n",
                "# model architectures, training parameters, and evaluation metrics.\n",
                "from config.config import (\n",
                "    DB_CONFIG,\n",
                "    MODEL_CONFIG,\n",
                "    TRAINING_CONFIG,\n",
                "    STRATEGY_CONFIG,\n",
                "    PATH_CONFIG,\n",
                "    EVAL_CONFIG,\n",
                "    HF_CONFIG,\n",
                "    ENV_CONFIG\n",
                ")\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "# This ensures that the experiment produces consistent results across multiple runs,\n",
                "# which is essential for debugging and comparing different approaches.\n",
                "random.seed(ENV_CONFIG[\"random_seed\"])\n",
                "np.random.seed(ENV_CONFIG[\"random_seed\"])\n",
                "torch.manual_seed(ENV_CONFIG[\"random_seed\"])\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(ENV_CONFIG[\"random_seed\"])\n",
                "\n",
                "# Determine the device to use for PyTorch computations (CPU or GPU)\n",
                "# If a GPU is available, it will be used to accelerate training.\n",
                "device = torch.device(TRAINING_CONFIG[\"device\"])\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Import project-specific modules\n",
                "# These modules contain the implementations of the various components of the system,\n",
                "# such as data loading, environment interaction, model architectures, and training algorithms.\n",
                "from src.data.database import DatabaseConnector\n",
                "from src.data.data_loader import JobRecommendationDataset, ReplayBuffer\n",
                "from src.environments.job_env import JobRecommendationEnv, LLMSimulatorEnv, HybridEnv\n",
                "from src.models.q_network import QNetwork\n",
                "from src.models.world_model import WorldModel\n",
                "from src.training.agent import DynaQAgent\n",
                "from src.utils.visualizer import Visualizer\n",
                "from src.utils.evaluator import Evaluator\n",
                "from src.data.tensor_cache import TensorCache\n",
                "\n",
                "print(\"Project modules imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "database-markdown"
            },
            "source": [
                "## 4. Database Connection\n",
                "\n",
                "The system connects to a MongoDB Atlas database containing job postings and candidate information. The database includes collections for:\n",
                "- Job text data (descriptions, requirements, etc.)\n",
                "- Job embedding vectors (semantic representations)\n",
                "- Candidate profiles\n",
                "- Candidate embedding vectors\n",
                "\n",
                "The connection is established using environment variables for credentials."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "database-code"
            },
            "outputs": [],
            "source": [
                "# Get reward strategy settings from config\n",
                "if not STRATEGY_CONFIG[\"hybrid\"][\"enabled\"]:\n",
                "    raise ValueError(\"Hybrid strategy is not enabled in config\")\n",
                "\n",
                "reward_strategy = \"hybrid\"  # Default to hybrid strategy\n",
                "cosine_weight = STRATEGY_CONFIG[\"hybrid\"][\"initial_cosine_weight\"]\n",
                "\n",
                "# Initialize database connector\n",
                "try:\n",
                "    db_connector = DatabaseConnector()  # Uses default connection string and db name from config\n",
                "    print(\"Database connection established successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Database connection error: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "data-loading-markdown"
            },
            "source": [
                "## 5. Data Loading\n",
                "\n",
                "We'll now access the database to:\n",
                "1. Retrieve candidate (applicant) embedding data\n",
                "2. Sample a subset of jobs from the database\n",
                "3. Retrieve job embedding vectors\n",
                "\n",
                "These operations use the DatabaseConnector methods to efficiently access the data needed for training and evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "data-loading-code-1"
            },
            "outputs": [],
            "source": [
                "# Query the database to get a list of candidate IDs\n",
                "try:\n",
                "    # Get the candidates_text collection\n",
                "    collection = db_connector.db[db_connector.collections[\"candidates_text\"]]\n",
                "    \n",
                "    # Query for candidate IDs (limit to 10 for demonstration)\n",
                "    candidate_ids = [doc[\"_id\"] for doc in collection.find({}, {\"_id\": 1}).limit(TRAINING_CONFIG[\"num_candidates\"])]\n",
                "    \n",
                "    if not candidate_ids:\n",
                "        raise ValueError(\"No candidates found in the database\")\n",
                "    \n",
                "    # Select the first candidate for demonstration\n",
                "    target_candidate_id = candidate_ids[0]\n",
                "    print(f\"Selected target candidate ID: {target_candidate_id}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error querying candidate IDs: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "data-loading-code-2"
            },
            "outputs": [],
            "source": [
                "# Get the candidate embedding directly from the database connector\n",
                "try:\n",
                "    candidate_embedding = db_connector.get_applicant_state(target_candidate_id)\n",
                "    print(f\"Candidate embedding shape: {candidate_embedding.shape}\")\n",
                "    \n",
                "    # Sample jobs from the database with embedding validation to ensure all have required fields\n",
                "    sampled_jobs = db_connector.sample_candidate_jobs(n=TRAINING_CONFIG[\"num_jobs\"], validate_embeddings=True)\n",
                "    job_ids = [job[\"_id\"] for job in sampled_jobs]\n",
                "    print(f\"Sampled {len(job_ids)} jobs from the database (all with valid embeddings)\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error retrieving candidate embedding or sampling jobs: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "data-loading-code-3"
            },
            "outputs": [],
            "source": [
                "# Get job vectors for the sampled jobs directly from the database connector\n",
                "try:\n",
                "    job_vectors = db_connector.get_job_vectors(job_ids)\n",
                "    print(f\"Fetched {len(job_vectors)} job vectors\")\n",
                "    \n",
                "    # Convert job vectors to NumPy array for easier handling\n",
                "    job_vectors_np = np.array([tensor.cpu().numpy() for tensor in job_vectors])\n",
                "    print(f\"Job vectors shape: {job_vectors_np.shape}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error retrieving job vectors: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "model-init-markdown"
            },
            "source": [
                "## 6. Model Initialization\n",
                "\n",
                "We'll initialize the neural networks used in the Dyna-Q algorithm:\n",
                "\n",
                "1. **Q-Network**: Approximates the value function mapping state-action pairs to expected returns\n",
                "2. **Target Q-Network**: A copy of the Q-Network used for stable learning\n",
                "3. **World Model**: Predicts next states and rewards based on current states and actions\n",
                "\n",
                "Each network is configured according to the parameters specified in the configuration file."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tensor-cache-markdown"
            },
            "source": [
                "## Tensor Cache Initialization\n",
                "\n",
                "Initialize the tensor cache to significantly speed up training by preloading all data from the database to GPU memory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "tensor-cache-code"
            },
            "outputs": [],
            "source": [
                "# Define cache configuration\n",
                "CACHE_CONFIG = {\n",
                "    \"enabled\": True,                                    # Whether to use tensor cache\n",
                "    \"device\": TRAINING_CONFIG.get(\"device\", \"cuda\"),    # Device to store tensors on\n",
                "    \"load_all_jobs\": True                               # Whether to load all valid jobs\n",
                "}\n",
                "\n",
                "if CACHE_CONFIG[\"enabled\"]:\n",
                "    print(f\"Initializing tensor cache on {CACHE_CONFIG['device']}...\")\n",
                "    \n",
                "    # Create tensor cache\n",
                "    tensor_cache = TensorCache(device=CACHE_CONFIG[\"device\"])\n",
                "    \n",
                "    # Load all data from database to cache\n",
                "    tensor_cache.copy_from_database(\n",
                "        db_connector=db_connector,\n",
                "        applicant_ids=[target_candidate_id]\n",
                "    )\n",
                "    \n",
                "    # Print cache statistics\n",
                "    stats = tensor_cache.cache_stats()\n",
                "    print(f\"Cache initialized with {stats['job_count']} jobs\")\n",
                "    print(f\"Initialization time: {stats['initialization_time']:.2f} seconds\")\n",
                "    print(f\"Memory device: {stats['device']}\")\n",
                "    \n",
                "    # Create environment with cache\n",
                "    if reward_strategy == \"cosine\":\n",
                "        env = JobRecommendationEnv(\n",
                "            db_connector=db_connector,\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_strategy=\"cosine\",\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    elif reward_strategy == \"llm\":\n",
                "        env = LLMSimulatorEnv(\n",
                "            db_connector=db_connector,\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"],\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    elif reward_strategy == \"hybrid\":\n",
                "        env = HybridEnv(\n",
                "            db_connector=db_connector,\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"],\n",
                "            cosine_weight=STRATEGY_CONFIG[\"hybrid\"][\"initial_cosine_weight\"],\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown reward strategy: {reward_strategy}\")\n",
                "    \n",
                "    print(f\"Created {reward_strategy} environment with tensor cache\")\n",
                "else:\n",
                "    # Create environment without cache\n",
                "    tensor_cache = None\n",
                "    if reward_strategy == \"cosine\":\n",
                "        env = JobRecommendationEnv(\n",
                "            db_connector=db_connector,\n",
                "            reward_strategy=\"cosine\",\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    elif reward_strategy == \"llm\":\n",
                "        env = LLMSimulatorEnv(\n",
                "            db_connector=db_connector,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"],\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    elif reward_strategy == \"hybrid\":\n",
                "        env = HybridEnv(\n",
                "            db_connector=db_connector,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"],\n",
                "            cosine_weight=STRATEGY_CONFIG[\"hybrid\"][\"initial_cosine_weight\"],\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown reward strategy: {reward_strategy}\")\n",
                "    \n",
                "    print(f\"Created {reward_strategy} environment without tensor cache\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model-init-code"
            },
            "outputs": [],
            "source": [
                "# Initialize Q-network\n",
                "q_network = QNetwork(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    hidden_dims=MODEL_CONFIG[\"q_network\"][\"hidden_dims\"],\n",
                "    dropout_rate=MODEL_CONFIG[\"q_network\"][\"dropout_rate\"],\n",
                "    activation=MODEL_CONFIG[\"q_network\"][\"activation\"]\n",
                ").to(device)\n",
                "\n",
                "# Initialize World Model\n",
                "world_model = WorldModel(\n",
                "    input_dim=MODEL_CONFIG[\"world_model\"][\"input_dim\"],\n",
                "    hidden_dims=MODEL_CONFIG[\"world_model\"][\"hidden_dims\"],\n",
                "    dropout_rate=MODEL_CONFIG[\"world_model\"][\"dropout_rate\"],\n",
                "    activation=MODEL_CONFIG[\"world_model\"][\"activation\"]\n",
                ").to(device)\n",
                "\n",
                "# Initialize DynaQAgent\n",
                "agent = DynaQAgent(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    q_network=q_network,\n",
                "    world_model=world_model,\n",
                "    training_strategy=reward_strategy,\n",
                "    device=device,\n",
                "    target_applicant_id=target_candidate_id,\n",
                "    tensor_cache=tensor_cache\n",
                ")\n",
                "\n",
                "# Initialize Visualizer\n",
                "visualizer = Visualizer()\n",
                "\n",
                "# Initialize Evaluator\n",
                "evaluator = Evaluator()\n",
                "\n",
                "# Set the target candidate ID for the environment\n",
                "env.reset(applicant_id=target_candidate_id)\n",
                "\n",
                "# Verify environment initialization\n",
                "if not hasattr(env, 'current_state'):\n",
                "    raise ValueError(\"Environment initialization failed - missing current_state\")\n",
                "\n",
                "print(f\"Environment initialized with {reward_strategy} reward strategy.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "llm-integration-markdown"
            },
            "source": [
                "## 7. LLM Integration (If Using LLM-Based Reward Strategy)\n",
                "\n",
                "If we're using an LLM-based reward strategy (either \"llm\" or \"hybrid\"), we need to set up a language model to simulate candidate responses to job recommendations. This helps in generating more realistic rewards based on semantic understanding of both candidate profiles and job descriptions.\n",
                "\n",
                "Note: This section will only execute if we're using an LLM-based strategy and running in a Colab environment with sufficient resources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "llm-integration-code"
            },
            "outputs": [],
            "source": [
                "# Set up LLM in the environment for reward calculation\n",
                "if STRATEGY_CONFIG[\"llm\"][\"enabled\"] and (reward_strategy == \"llm\" or reward_strategy == \"hybrid\"):\n",
                "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "    from huggingface_hub import login\n",
                "    \n",
                "    # Get token path from config and read token\n",
                "    token_path = HF_CONFIG[\"token_path\"]\n",
                "    if not os.path.isabs(token_path):\n",
                "        token_path = token_path\n",
                "    with open(token_path, \"r\") as f:\n",
                "        token = f.read().strip()\n",
                "    \n",
                "    # Login to Hugging Face\n",
                "    login(token=token)\n",
                "    print(f\"Successfully logged in to Hugging Face using token from {token_path}\")\n",
                "    \n",
                "    # Load model and tokenizer\n",
                "    model_id = STRATEGY_CONFIG[\"llm\"][\"model_id\"]\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"load_in_4bit\"],\n",
                "        bnb_4bit_quant_type=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"quant_type\"],\n",
                "        bnb_4bit_use_nested_quant=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"use_nested_quant\"],\n",
                "        bnb_4bit_compute_dtype=torch.bfloat16 \n",
                "    )\n",
                "    \n",
                "    print(f\"Loading tokenizer for model: {model_id}\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "    \n",
                "    print(f\"Loading model {model_id} with 4-bit quantization...\")\n",
                "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_id,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    \n",
                "    print(f\"LLM model loaded successfully: {model_id}\")\n",
                "    \n",
                "    # Add LLM to the environment\n",
                "    env.setup_llm(llm_model, tokenizer)\n",
                "    print(f\"Added LLM to {reward_strategy} environment\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "training-setup-markdown"
            },
            "source": [
                "## 8. Training Setup\n",
                "\n",
                "We'll now set up the training process using the DynaQAgent's built-in training interface.\n",
                "The agent handles:\n",
                "1. Experience replay buffer\n",
                "2. Q-Network and World Model updates\n",
                "3. Loss tracking and metrics\n",
                "4. Model checkpointing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training-setup-code"
            },
            "outputs": [],
            "source": [
                "# Verify the agent is properly set up\n",
                "print(f\"Agent device: {agent.device}\")\n",
                "print(f\"Training strategy: {agent.training_strategy}\")\n",
                "print(f\"Q-Network parameters: {sum(p.numel() for p in agent.q_network.parameters() if p.requires_grad):,}\")\n",
                "print(f\"World Model parameters: {sum(p.numel() for p in agent.world_model.parameters() if p.requires_grad):,}\")\n",
                "print(f\"Planning steps: {TRAINING_CONFIG['planning_steps']}\")\n",
                "print(f\"Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
                "\n",
                "# Configure any additional training parameters\n",
                "agent.planning_steps = TRAINING_CONFIG['planning_steps']\n",
                "agent.batch_size = TRAINING_CONFIG['batch_size']\n",
                "agent.gamma = TRAINING_CONFIG['gamma']\n",
                "\n",
                "print(\"Training setup completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "pretraining-markdown"
            },
            "source": [
                "## 9. Pretraining Phase\n",
                "\n",
                "In the pretraining phase, we'll use the agent's built-in pretraining method to:\n",
                "1. Generate initial experiences\n",
                "2. Train the Q-network and world model\n",
                "3. Track training metrics\n",
                "4. Save model checkpoints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "pretraining-code"
            },
            "outputs": [],
            "source": [
                "# Define pretraining parameters from config\n",
                "num_pretraining_samples = TRAINING_CONFIG[\"pretraining\"][\"num_samples\"]\n",
                "num_pretraining_epochs = TRAINING_CONFIG[\"pretraining\"][\"num_epochs\"]\n",
                "batch_size = TRAINING_CONFIG[\"batch_size\"]\n",
                "\n",
                "# Use the configured reward strategy for pretraining\n",
                "pretraining_strategy = reward_strategy\n",
                "\n",
                "print(f\"Starting pretraining with {pretraining_strategy} strategy...\")\n",
                "\n",
                "# Generate pretraining data using tensor cache\n",
                "print(\"Generating pretraining data using tensor cache\")\n",
                "pretraining_data = []\n",
                "state = env.reset(applicant_id=target_candidate_id)\n",
                "\n",
                "# Get valid job indices from tensor cache\n",
                "valid_job_indices = tensor_cache.get_valid_job_indices()\n",
                "print(f\"Using {len(valid_job_indices)} valid jobs from tensor cache for pretraining\")\n",
                "\n",
                "# Get the valid actions from the environment to ensure indices match\n",
                "valid_actions = env.get_valid_actions()\n",
                "print(f\"Environment has {len(valid_actions)} valid actions\")\n",
                "\n",
                "# Create a mapping from tensor cache indices to environment action indices\n",
                "job_id_to_action_map = {}\n",
                "for action_idx in valid_actions:\n",
                "    job = env.candidate_jobs[action_idx]\n",
                "    job_id = job.get(\"_id\")\n",
                "    if job_id in tensor_cache.job_ids:\n",
                "        cache_idx = tensor_cache.job_ids.index(job_id)\n",
                "        job_id_to_action_map[cache_idx] = action_idx\n",
                "        print(f\"Mapped tensor cache job {job_id} (index {cache_idx}) to environment action {action_idx}\")\n",
                "\n",
                "print(f\"Created mapping for {len(job_id_to_action_map)} jobs between tensor cache and environment\")\n",
                "\n",
                "# Collect pretraining experiences\n",
                "samples_collected = 0\n",
                "max_attempts = min(num_pretraining_samples * 2, len(valid_job_indices))\n",
                "\n",
                "for i in tqdm(range(max_attempts), desc=\"Generating pretraining data\"):\n",
                "    if samples_collected >= num_pretraining_samples:\n",
                "        break\n",
                "        \n",
                "    cache_job_idx = valid_job_indices[i % len(valid_job_indices)]\n",
                "    if cache_job_idx not in job_id_to_action_map:\n",
                "        continue\n",
                "    \n",
                "    action_idx = job_id_to_action_map[cache_job_idx]\n",
                "    next_state, reward, done, _ = env.step(action_idx)\n",
                "    job_vector = tensor_cache.get_job_vector_by_index(cache_job_idx)\n",
                "    pretraining_data.append((state, job_vector, reward, next_state))\n",
                "    state = next_state\n",
                "    samples_collected += 1\n",
                "\n",
                "print(f\"Collected {len(pretraining_data)} pretraining experiences\")\n",
                "\n",
                "# Create tensor batches\n",
                "states = torch.stack([d[0] for d in pretraining_data])\n",
                "actions = torch.stack([d[1] for d in pretraining_data])\n",
                "rewards = torch.tensor([d[2] for d in pretraining_data], device=device)\n",
                "next_states = torch.stack([d[3] for d in pretraining_data])\n",
                "\n",
                "# Run pretraining using agent's built-in pretrain method\n",
                "print(\"Starting pretraining of neural networks\")\n",
                "pretraining_metrics = agent.pretrain(\n",
                "    states=states,\n",
                "    actions=actions,\n",
                "    rewards=rewards,\n",
                "    next_states=next_states,\n",
                "    num_epochs=num_pretraining_epochs,\n",
                "    batch_size=batch_size\n",
                ")\n",
                "\n",
                "# Plot pretraining metrics\n",
                "visualizer.plot_training_metrics(\n",
                "    metrics={\n",
                "        'q_losses': pretraining_metrics['q_losses'],\n",
                "        'world_losses': pretraining_metrics['world_losses']\n",
                "    },\n",
                "    title=\"Pretraining Losses\"\n",
                ")\n",
                "\n",
                "# Configure agent for main training\n",
                "agent.planning_steps = TRAINING_CONFIG['planning_steps']\n",
                "agent.batch_size = TRAINING_CONFIG['batch_size']\n",
                "agent.gamma = TRAINING_CONFIG['gamma']\n",
                "\n",
                "print(\"Pretraining completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "main-training-markdown"
            },
            "source": [
                "## 10. Main Training Loop\n",
                "\n",
                "Now we'll run the main training loop using the agent's training interface.\n",
                "The agent will:\n",
                "1. Interact with the environment\n",
                "2. Store experiences in its replay buffer\n",
                "3. Update networks using the stored experiences\n",
                "4. Track and report training metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "main-training-code"
            },
            "outputs": [],
            "source": [
                "# Training parameters from config\n",
                "num_episodes = TRAINING_CONFIG[\"num_episodes\"]\n",
                "max_steps_per_episode = TRAINING_CONFIG[\"max_steps_per_episode\"]\n",
                "\n",
                "print(\"Starting main training loop...\")\n",
                "\n",
                "# Set up for multiple experiments\n",
                "num_experiments = 5  # Number of experiments to run\n",
                "experiment_results = {\n",
                "    'q_network_loss': [],\n",
                "    'world_model_loss': [],\n",
                "    'episode_reward': [],\n",
                "    'eval_reward': []\n",
                "}\n",
                "\n",
                "# Create directory for saving results\n",
                "results_dir = os.path.join(PATH_CONFIG[\"results_dir\"], \"multi_experiment\")\n",
                "os.makedirs(results_dir, exist_ok=True)\n",
                "\n",
                "# Run multiple experiments\n",
                "for exp_idx in range(num_experiments):\n",
                "    print(f\"\\nStarting experiment {exp_idx+1}/{num_experiments}\")\n",
                "    \n",
                "    # Reinitialize agent for each experiment to ensure independence\n",
                "    if exp_idx > 0:\n",
                "        q_network = QNetwork(\n",
                "            state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "            action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "            hidden_dims=MODEL_CONFIG[\"q_network\"][\"hidden_dims\"],\n",
                "            dropout_rate=MODEL_CONFIG[\"q_network\"][\"dropout_rate\"],\n",
                "            activation=MODEL_CONFIG[\"q_network\"][\"activation\"]\n",
                "        ).to(device)\n",
                "\n",
                "        world_model = WorldModel(\n",
                "            input_dim=MODEL_CONFIG[\"world_model\"][\"input_dim\"],\n",
                "            hidden_dims=MODEL_CONFIG[\"world_model\"][\"hidden_dims\"],\n",
                "            dropout_rate=MODEL_CONFIG[\"world_model\"][\"dropout_rate\"],\n",
                "            activation=MODEL_CONFIG[\"world_model\"][\"activation\"]\n",
                "        ).to(device)\n",
                "        \n",
                "        agent = DynaQAgent(\n",
                "            state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "            action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "            q_network=q_network,\n",
                "            world_model=world_model,\n",
                "            training_strategy=reward_strategy,\n",
                "            device=device,\n",
                "            target_applicant_id=target_candidate_id,\n",
                "            tensor_cache=tensor_cache\n",
                "        )\n",
                "    \n",
                "    # Run training for this experiment\n",
                "    training_metrics = agent.train(\n",
                "        env=env,\n",
                "        num_episodes=num_episodes,\n",
                "        max_steps_per_episode=max_steps_per_episode,\n",
                "        applicant_ids=[target_candidate_id]\n",
                "    )\n",
                "    \n",
                "    # Store results for this experiment\n",
                "    for key in experiment_results:\n",
                "        if key in training_metrics:\n",
                "            experiment_results[key].append(training_metrics[key])\n",
                "    \n",
                "    # Save individual experiment results\n",
                "    exp_file = os.path.join(results_dir, f\"experiment_{exp_idx+1}.pt\")\n",
                "    torch.save(training_metrics, exp_file)\n",
                "    print(f\"Saved experiment {exp_idx+1} results to {exp_file}\")\n",
                "\n",
                "# Save aggregated results\n",
                "aggregated_file = os.path.join(results_dir, \"aggregated_results.pt\")\n",
                "torch.save(experiment_results, aggregated_file)\n",
                "print(f\"Saved aggregated results to {aggregated_file}\")\n",
                "\n",
                "# Visualize results using the Visualizer\n",
                "visualizer.plot_experiment_results(\n",
                "    experiment_results=experiment_results,\n",
                "    title_prefix=\"Dyna-Q Performance\",\n",
                "    filename_prefix=\"multi_experiment\"\n",
                ")\n",
                "\n",
                "# Plot training metrics for the last experiment\n",
                "visualizer.plot_training_metrics(\n",
                "    metrics={\n",
                "        'q_losses': training_metrics['q_network_loss'],\n",
                "        'world_losses': training_metrics['world_model_loss'],\n",
                "        'episode_rewards': training_metrics['episode_reward']\n",
                "    },\n",
                "    title=\"Training Metrics (Last Experiment)\"\n",
                ")\n",
                "\n",
                "print(\"Multiple experiments completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "results-loading-markdown"
            },
            "source": [
                "## 10.1 Loading and Visualizing Saved Experiment Results\n",
                "\n",
                "This section provides functionality to load and analyze results from previously run experiments.\n",
                "This is useful for revisiting experiment results without having to rerun the experiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "results-loading-code"
            },
            "outputs": [],
            "source": [
                "def load_experiment_results(results_dir):\n",
                "    \"\"\"\n",
                "    Load saved experiment results for visualization.\n",
                "    \n",
                "    Args:\n",
                "        results_dir: Directory containing experiment results\n",
                "    \n",
                "    Returns:\n",
                "        dict: Loaded experiment results\n",
                "    \"\"\"\n",
                "    # Check if aggregated results exist\n",
                "    aggregated_file = os.path.join(results_dir, \"aggregated_results.pt\")\n",
                "    \n",
                "    if os.path.exists(aggregated_file):\n",
                "        print(f\"Loading aggregated results from {aggregated_file}\")\n",
                "        return torch.load(aggregated_file)\n",
                "    else:\n",
                "        # Try to load individual experiment files\n",
                "        import glob\n",
                "        exp_files = glob.glob(os.path.join(results_dir, \"experiment_*.pt\"))\n",
                "        \n",
                "        if not exp_files:\n",
                "            print(f\"No experiment files found in {results_dir}\")\n",
                "            return {}\n",
                "        \n",
                "        print(f\"Found {len(exp_files)} individual experiment files\")\n",
                "        \n",
                "        # Load individual files and combine them\n",
                "        combined_results = {\n",
                "            'q_network_loss': [],\n",
                "            'world_model_loss': [],\n",
                "            'episode_reward': [],\n",
                "            'eval_reward': []\n",
                "        }\n",
                "        \n",
                "        for file in exp_files:\n",
                "            exp_data = torch.load(file)\n",
                "            for key in combined_results:\n",
                "                if key in exp_data:\n",
                "                    combined_results[key].append(exp_data[key])\n",
                "        \n",
                "        return combined_results\n",
                "\n",
                "# Example usage (commented out by default)\n",
                "# results_dir = os.path.join(PATH_CONFIG[\"results_dir\"], \"multi_experiment\")\n",
                "# loaded_results = load_experiment_results(results_dir)\n",
                "# if loaded_results:\n",
                "#     visualizer.plot_experiment_results(\n",
                "#         experiment_results=loaded_results,\n",
                "#         title_prefix=\"Loaded Results\",\n",
                "#         filename_prefix=\"loaded_experiment\"\n",
                "#     )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "evaluation-markdown"
            },
            "source": [
                "## 11. Evaluation\n",
                "\n",
                "Finally, we'll evaluate the trained agent using the Evaluator class.\n",
                "This will:\n",
                "1. Test the agent on a set of evaluation episodes\n",
                "2. Compare performance against baseline\n",
                "3. Generate evaluation metrics and visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "evaluation-code"
            },
            "outputs": [],
            "source": [
                "# Evaluation parameters from config\n",
                "num_eval_episodes = EVAL_CONFIG[\"num_episodes\"]\n",
                "baseline_strategy = EVAL_CONFIG[\"baseline_strategy\"]\n",
                "\n",
                "print(\"Starting evaluation...\")\n",
                "\n",
                "# Create a baseline agent for comparison\n",
                "baseline_agent = DynaQAgent(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    training_strategy=baseline_strategy,\n",
                "    device=device,\n",
                "    tensor_cache=tensor_cache  # Include tensor_cache\n",
                ")\n",
                "\n",
                "# Use evaluator's compare_agents method which internally calls evaluate_agent\n",
                "comparison_results = evaluator.compare_agents(\n",
                "    baseline_agent=baseline_agent,\n",
                "    pretrained_agent=agent,\n",
                "    env=env,\n",
                "    applicant_ids=[target_candidate_id],\n",
                "    num_episodes=num_eval_episodes\n",
                ")\n",
                "\n",
                "# Extract the results for visualization\n",
                "evaluation_results = {\n",
                "    'agent_rewards': comparison_results['pretrained']['episode_rewards'],\n",
                "    'baseline_rewards': comparison_results['baseline']['episode_rewards']\n",
                "}\n",
                "\n",
                "# Plot evaluation results\n",
                "visualizer.plot_evaluation_results(\n",
                "    baseline_rewards=evaluation_results['baseline_rewards'],\n",
                "    pretrained_rewards=evaluation_results['agent_rewards'],\n",
                "    title=\"Evaluation Results\"\n",
                ")\n",
                "\n",
                "print(\"Evaluation completed successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "recommendations-markdown"
            },
            "source": [
                "## 12. Generate Job Recommendations\n",
                "\n",
                "Now let's use our trained agent to generate personalized job recommendations for our target candidate. We'll:\n",
                "\n",
                "1. Use the trained Q-network to evaluate jobs\n",
                "2. Select the top-K jobs with highest Q-values\n",
                "3. Display the recommendations along with job details\n",
                "\n",
                "These recommendations represent the jobs the agent believes are most suitable for the candidate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "recommendations-code"
            },
            "outputs": [],
            "source": [
                "# Define testing parameters\n",
                "num_recommendations = EVAL_CONFIG[\"top_k_recommendations\"]\n",
                "test_epsilon = 0.0  # No exploration during testing (pure exploitation)\n",
                "\n",
                "# Initialize lists to store recommendations\n",
                "recommended_jobs = []\n",
                "recommendation_scores = []\n",
                "\n",
                "# Get all valid job indices\n",
                "valid_job_indices = list(range(len(tensor_cache.job_ids)))\n",
                "print(f\"Found {len(valid_job_indices)} valid jobs in tensor cache for recommendations\")\n",
                "\n",
                "# Reset the environment to get the initial state\n",
                "state = env.reset(applicant_id=target_candidate_id)\n",
                "\n",
                "# Create a copy of job indices to work with\n",
                "remaining_job_indices = valid_job_indices.copy()\n",
                "\n",
                "# Generate top-K recommendations\n",
                "for _ in range(min(num_recommendations, len(valid_job_indices))):\n",
                "    # Get job tensors for remaining indices\n",
                "    action_tensors = [tensor_cache.job_vectors[idx] for idx in remaining_job_indices]\n",
                "    \n",
                "    # Select best job according to Q-network\n",
                "    action_idx, _ = agent.select_action(state, action_tensors, eval_mode=True)\n",
                "    \n",
                "    # Get the corresponding job_id\n",
                "    selected_idx = remaining_job_indices[action_idx]\n",
                "    job_id = tensor_cache.job_ids[selected_idx]\n",
                "    \n",
                "    # Calculate Q-value for logging\n",
                "    with torch.no_grad():\n",
                "        state_tensor = torch.tensor(state, device=device).unsqueeze(0)\n",
                "        job_tensor = tensor_cache.job_vectors[selected_idx].unsqueeze(0)\n",
                "        q_value = agent.q_network(state_tensor, job_tensor).item()\n",
                "    \n",
                "    recommended_jobs.append(job_id)\n",
                "    recommendation_scores.append(q_value)\n",
                "    \n",
                "    # Remove selected job from consideration for next selection\n",
                "    remaining_job_indices.pop(action_idx)\n",
                "\n",
                "# Display recommendations with details\n",
                "print(\"\\n=== Top Job Recommendations ===\\n\")\n",
                "for i, (job_id, score) in enumerate(zip(recommended_jobs, recommendation_scores)):\n",
                "    print(f\"Recommendation #{i+1}: [Q-Value: {score:.4f}]\")\n",
                "    print(f\"Job ID: {job_id}\")\n",
                "    \n",
                "    # Retrieve and display job details\n",
                "    try:\n",
                "        job_details = db_connector.get_job_details(job_id)\n",
                "        print(f\"Title: {job_details.get('job_title', 'N/A')}\")\n",
                "        \n",
                "        # Display truncated description\n",
                "        description = job_details.get('description', 'N/A')\n",
                "        print(f\"Description: {description[:100]}...\" if len(description) > 100 else f\"Description: {description}\")\n",
                "        \n",
                "        # Display technical skills if available\n",
                "        if 'technical_skills' in job_details:\n",
                "            print(f\"Technical Skills: {', '.join(job_details['technical_skills'])}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error retrieving job details: {e}\")\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "conclusion-markdown"
            },
            "source": [
                "## 13. Conclusion and Next Steps\n",
                "\n",
                "In this notebook, we've implemented and demonstrated a Neural Dyna-Q job recommendation system. This approach combines the strengths of deep reinforcement learning with model-based planning to provide personalized job recommendations.\n",
                "\n",
                "### 13.1 Key Accomplishments\n",
                "\n",
                "1. **Data Integration**: Connected to the MongoDB database to retrieve real candidate and job data\n",
                "2. **Neural Networks**: Implemented deep Q-network and world model for value function and dynamics prediction\n",
                "3. **Dyna-Q Algorithm**: Combined direct RL with model-based planning for efficient learning\n",
                "4. **Personalized Recommendations**: Generated job recommendations tailored to a specific candidate\n",
                "\n",
                "### 13.2 Potential Improvements\n",
                "\n",
                "1. **Extended Training**: Train for more episodes to improve recommendation quality\n",
                "2. **Hyperparameter Tuning**: Optimize learning rates, network architectures, and other parameters\n",
                "3. **Advanced Reward Functions**: Implement more sophisticated reward strategies using LLMs\n",
                "4. **User Feedback**: Incorporate real user feedback to improve recommendations\n",
                "\n",
                "### 13.3 Applications\n",
                "\n",
                "This system could be deployed as:\n",
                "- A personalized job recommendation service for job seekers\n",
                "- A candidate-job matching tool for recruiters\n",
                "- A component in a larger career guidance system"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cleanup-code"
            },
            "outputs": [],
            "source": [
                "# Clean up resources\n",
                "db_connector.close()\n",
                "print(\"Database connection closed.\")\n",
                "\n",
                "# Free up GPU memory\n",
                "if tensor_cache is not None and hasattr(tensor_cache, 'clear'):\n",
                "    tensor_cache.clear()\n",
                "    print(\"Tensor cache cleared.\")\n",
                "    \n",
                "# Free PyTorch memory\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()\n",
                "    print(\"CUDA cache emptied.\")\n",
                "    \n",
                "print(\"Notebook execution complete.\")"
            ]
        }
    ]
}
   