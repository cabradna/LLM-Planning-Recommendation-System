{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "colab": {
            "provenance": [],
            "include_colab_link": true
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-000"
            },
            "source": [
                "# Neural Dyna-Q for Job Recommendation: Model-Based RL Approach\n",
                "\n",
                "**Author:** LLM Lab Team  \n",
                "**Date:** 2023-10-15\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-001"
            },
            "source": [
                "## 1. Introduction and Overview\n",
                "\n",
                "This notebook demonstrates a Neural Dyna-Q approach to personalized job recommendation. By combining deep reinforcement learning with model-based planning, we build a system that learns to recommend jobs that match candidate skills and preferences.\n",
                "\n",
                "### 1.1 Core Algorithm Concepts\n",
                "\n",
                "The Neural Dyna-Q algorithm integrates these key components:\n",
                "\n",
                "- **Q-Learning**: Learning a value function that predicts the utility of job recommendations\n",
                "- **World Model**: A neural network that models the environment dynamics\n",
                "- **Planning**: Using the world model to simulate experiences and improve the value function\n",
                "- **Exploration Strategy**: Balancing exploration and exploitation with epsilon-greedy policy\n",
                "\n",
                "### 1.2 Key Features of Implementation\n",
                "\n",
                "- **Single-Applicant Specialization**: Each agent is trained for one specific applicant\n",
                "- **Neural Networks**: Deep learning for value function and environment dynamics approximation\n",
                "- **Multiple Reward Generation Strategies**: Cosine similarity, LLM feedback, and hybrid approaches\n",
                "- **Dyna-Q Algorithm**: Combines direct RL with model-based planning\n",
                "- **Tensor-Based Caching**: Efficient GPU-accelerated data access for faster training\n",
                "\n",
                "### 1.3 Expected Outcomes\n",
                "\n",
                "By the end of this notebook, we'll have:\n",
                "1. A trained job recommendation agent for a specific candidate\n",
                "2. Visualizations of the learning process\n",
                "3. A list of personalized job recommendations\n",
                "4. Insights into the model's performance\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-002"
            },
            "source": [
                "## 2. Environment Setup\n",
                "\n",
                "First, we'll set up our environment, install necessary packages, and import libraries. If running in Google Colab, we'll clone the repository and install dependencies.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-003"
            },
            "source": [
                "# Check if running in Google Colab\n",
                "# Import necessary libraries for setup\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuration settings for the setup process, particularly for Colab environments.\n",
                "SETUP_PATH_CONFIG = {\n",
                "    \"repo_url\": \"https://github.com/cabradna/LLM-Planning-Recommendation-System.git\",  # URL of the repository to clone.\n",
                "    \"branch\": \"local_tensors\"  # Specific branch to use for tensor-based implementation\n",
                "}\n",
                "\n",
                "# Configuration for enabling specific strategies during setup (e.g., LLM, hybrid).\n",
                "SETUP_STRATEGY_CONFIG = {\n",
                "    \"llm\": {\"enabled\": True},  # Enable LLM-related dependency installation.\n",
                "    \"hybrid\": {\"enabled\": False} # Enable hybrid strategy dependency installation.\n",
                "}\n",
                "\n",
                "# Determine if the code is running in Google Colab.\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"Running in Google Colab. Cloning repository and installing dependencies...\")\n",
                "\n",
                "    try:\n",
                "        # Retrieve repository URL from configuration.\n",
                "        repo_url = SETUP_PATH_CONFIG[\"repo_url\"]\n",
                "        repo_name = Path(repo_url).stem  # Extract repository name from the URL.\n",
                "\n",
                "        # Clone the repository.\n",
                "        print(f\"Cloning repository: {repo_url}\")\n",
                "        subprocess.run([\"git\", \"clone\", \"-q\", repo_url], check=True)  # Clone quietly and raise an error if cloning fails.\n",
                "\n",
                "        # Navigate into the cloned repository directory.\n",
                "        cloned_repo_path = Path(repo_name)\n",
                "        if cloned_repo_path.is_dir():\n",
                "            os.chdir(cloned_repo_path)\n",
                "            print(f\"Changed directory to: {os.getcwd()}\")\n",
                "            \n",
                "            # Checkout the specific branch\n",
                "            branch = SETUP_PATH_CONFIG[\"branch\"]\n",
                "            print(f\"Checking out branch: {branch}\")\n",
                "            subprocess.run([\"git\", \"checkout\", branch], check=True)\n",
                "        else:\n",
                "            raise FileNotFoundError(f\"Cloned repository directory '{repo_name}' not found or is not a directory.\")\n",
                "\n",
                "        # Install base requirements from requirements.txt.\n",
                "        requirements_path = Path(\"requirements.txt\")\n",
                "        if requirements_path.is_file():\n",
                "            print(\"Installing base requirements from requirements.txt...\")\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_path)], check=True)  # Use the correct pip and install quietly.\n",
                "        else:\n",
                "            print(\"Warning: requirements.txt not found. Skipping base requirements installation.\")\n",
                "\n",
                "        # Install the project package in editable mode.\n",
                "        print(\"Attempting to install project package in editable mode...\")\n",
                "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"], check=True)  # Install the current package in editable mode.\n",
                "\n",
                "        # Install extra packages if LLM or hybrid strategy is enabled.\n",
                "        if SETUP_STRATEGY_CONFIG[\"llm\"][\"enabled\"] or SETUP_STRATEGY_CONFIG[\"hybrid\"][\"enabled\"]:\n",
                "            print(\"Installing LLM-related packages (transformers, accelerate, bitsandbytes)...\")\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"accelerate\"], check=True)\n",
                "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bitsandbytes>=0.45.0\"], check=True)\n",
                "            print(\"LLM-related packages installed successfully.\")\n",
                "\n",
                "        print(\"Repository cloned and dependencies installed successfully.\")\n",
                "\n",
                "        # Add the project root to the Python path.\n",
                "        project_root = Path(os.getcwd()).absolute()\n",
                "        if str(project_root) not in sys.path:\n",
                "            sys.path.insert(0, str(project_root))  # Insert at the beginning to prioritize project modules.\n",
                "            print(f\"Added {project_root} to sys.path\")\n",
                "\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"Setup command failed: {e}\")\n",
                "        print(\"Please check the repository URL, requirements file, setup.py, and Colab environment.\")\n",
                "        raise  # Re-raise the exception to halt execution.\n",
                "    except FileNotFoundError as e:\n",
                "        print(f\"Setup error: {e}\")\n",
                "        raise  # Re-raise the exception to halt execution.\n",
                "else:\n",
                "    print(\"Not running in Google Colab. Assuming local environment is set up.\")\n",
                "\n",
                "    try:\n",
                "        # Determine the project root directory.\n",
                "        project_root = Path(__file__).resolve().parent.parent\n",
                "    except NameError:\n",
                "        # Handle cases where __file__ is not defined (e.g., interactive environments).\n",
                "        project_root = Path('.').absolute()\n",
                "        print(f\"Warning: __file__ not defined. Assuming project root is CWD: {project_root}\")\n",
                "\n",
                "    # Add the project root to the Python path if it's not already there.\n",
                "    if str(project_root) not in sys.path:\n",
                "        sys.path.insert(0, str(project_root))\n",
                "        print(f\"Added {project_root} to sys.path for local execution.\")\n",
                "        \n",
                "    # Verify we're on the correct branch\n",
                "    try:\n",
                "        result = subprocess.run([\"git\", \"branch\", \"--show-current\"], capture_output=True, text=True)\n",
                "        current_branch = result.stdout.strip()\n",
                "        if current_branch != SETUP_PATH_CONFIG[\"branch\"]:\n",
                "            print(f\"Warning: Current branch is '{current_branch}', but code expects '{SETUP_PATH_CONFIG['branch']}'\")\n",
                "            print(\"Please switch to the correct branch for this implementation.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not verify git branch: {e}\")\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-004"
            },
            "source": [
                "# Standard imports for data manipulation, visualization, and deep learning\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import random\n",
                "from tqdm.notebook import tqdm  # Use tqdm.notebook for Colab/notebook progress bars\n",
                "import torch\n",
                "from pathlib import Path\n",
                "\n",
                "# --- Configure Logging --- \n",
                "import logging\n",
                "from config.config import PATH_CONFIG # Assuming PATH_CONFIG is defined here\n",
                "\n",
                "# Define log levels\n",
                "file_log_level = logging.DEBUG  # Always log DEBUG and above to file\n",
                "initial_stream_log_level = logging.INFO # Initial level for notebook output\n",
                "\n",
                "# Ensure log directory exists\n",
                "log_dir = PATH_CONFIG.get(\"log_dir\", \"logs\") # Default to ./logs if not in config\n",
                "if not os.path.isabs(log_dir):\n",
                "    try:\n",
                "        project_root_for_log = Path(__file__).resolve().parent.parent\n",
                "    except NameError:\n",
                "        project_root_for_log = Path('.').absolute()\n",
                "    log_dir = os.path.join(project_root_for_log, log_dir)\n",
                "os.makedirs(log_dir, exist_ok=True)\n",
                "log_file_path = os.path.join(log_dir, 'notebook_run.log')\n",
                "\n",
                "# Get root logger and set level to lowest required (DEBUG)\n",
                "logger = logging.getLogger()\n",
                "logger.setLevel(logging.DEBUG) # Allow all messages >= DEBUG through the root logger\n",
                "\n",
                "# Remove existing handlers from previous runs (if any)\n",
                "# This is important if re-running this cell in the notebook\n",
                "for handler in logger.handlers[:]:\n",
                "   logger.removeHandler(handler)\n",
                "\n",
                "# Create formatter\n",
                "log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
                "\n",
                "# Create File Handler (Level: DEBUG)\n",
                "file_handler = logging.FileHandler(log_file_path, mode='w') # 'w' to overwrite each run\n",
                "file_handler.setFormatter(log_formatter)\n",
                "file_handler.setLevel(file_log_level) # Log DEBUG and up to the file\n",
                "logger.addHandler(file_handler)\n",
                "\n",
                "# Create Stream Handler (Level: INFO initially)\n",
                "# Store the handler instance in a variable so its level can be changed later\n",
                "stream_handler = logging.StreamHandler(sys.stdout)\n",
                "stream_handler.setFormatter(log_formatter)\n",
                "stream_handler.setLevel(initial_stream_log_level) # Log INFO and up to the console/notebook\n",
                "logger.addHandler(stream_handler)\n",
                "\n",
                "logger.info(f\"Logging configured. Root Level: DEBUG. File Level: {logging.getLevelName(file_log_level)}. Stream Level: {logging.getLevelName(initial_stream_log_level)}. File: {log_file_path}\")\n",
                "logger.debug(\"DEBUG logging is enabled for the root logger and file handler.\") # This will appear in file only (unless stream level is changed)\n",
                "\n",
                "# --- How to change Stream Handler level later --- \n",
                "# In a later cell, to see DEBUG messages in notebook output:\n",
                "# stream_handler.setLevel(logging.DEBUG)\n",
                "# logger.info(\"--- Changed Stream Handler level to DEBUG ---\")\n",
                "\n",
                "# To switch Stream Handler back to INFO:\n",
                "# stream_handler.setLevel(logging.INFO)\n",
                "# logger.info(\"--- Changed Stream Handler level back to INFO ---\")\n",
                "# --- End Logging Configuration ---\n",
                "\n",
                "# Dynamically add the project root to the Python path\n",
                "# This allows importing modules from the project's source code, regardless of the current working directory.\n",
                "try:\n",
                "    # Determine the project root directory based on the location of this script.\n",
                "    # Assumes the script is located in a subdirectory of the project root.\n",
                "    project_root = Path(__file__).resolve().parent.parent\n",
                "except NameError:\n",
                "    # Handle cases where __file__ is not defined, such as in interactive environments.\n",
                "    project_root = Path('.').absolute()\n",
                "\n",
                "# Add the project root to the Python path if it's not already there.\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))  # Prioritize project modules over installed ones\n",
                "\n",
                "# Import configuration settings from the project's config module\n",
                "# These settings define various aspects of the experiment, such as database connections,\n",
                "# model architectures, training parameters, and evaluation metrics.\n",
                "from config.config import (\n",
                "    DB_CONFIG,\n",
                "    MODEL_CONFIG,\n",
                "    TRAINING_CONFIG,\n",
                "    STRATEGY_CONFIG,\n",
                "    PATH_CONFIG,\n",
                "    EVAL_CONFIG,\n",
                "    HF_CONFIG,\n",
                "    ENV_CONFIG,\n",
                "    PRETRAINING_CONFIG\n",
                ")\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "random.seed(ENV_CONFIG[\"random_seed\"])\n",
                "np.random.seed(ENV_CONFIG[\"random_seed\"])\n",
                "torch.manual_seed(ENV_CONFIG[\"random_seed\"])\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(ENV_CONFIG[\"random_seed\"])\n",
                "\n",
                "# Determine the device to use for PyTorch computations (CPU or GPU)\n",
                "# If a GPU is available, it will be used to accelerate training.\n",
                "device = torch.device(TRAINING_CONFIG[\"device\"])\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Import project-specific modules\n",
                "# These modules contain the implementations of the various components of the system,\n",
                "# such as data loading, environment interaction, model architectures, and training algorithms.\n",
                "from src.data.database import DatabaseConnector\n",
                "from src.data.data_loader import JobRecommendationDataset, ReplayBuffer\n",
                "from src.environments.job_env import JobRecommendationEnv, LLMSimulatorEnv, HybridEnv\n",
                "from src.models.q_network import QNetwork\n",
                "from src.models.world_model import WorldModel\n",
                "from src.training.agent import DynaQAgent\n",
                "from src.utils.visualizer import Visualizer\n",
                "from src.utils.evaluator import Evaluator\n",
                "from src.data.tensor_cache import TensorCache\n",
                "\n",
                "print(\"Project modules imported successfully.\")\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-005"
            },
            "source": [
                "## 3. Database Connection\n",
                "\n",
                "The system connects to a MongoDB Atlas database containing job postings and candidate information. The database includes collections for:\n",
                "- Job text data (descriptions, requirements, etc.)\n",
                "- Job embedding vectors (semantic representations)\n",
                "- Candidate profiles\n",
                "- Candidate embedding vectors\n",
                "\n",
                "The connection is established using environment variables for credentials."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-006"
            },
            "source": [
                "# Get reward strategy settings from config\n",
                "if not STRATEGY_CONFIG[\"hybrid\"][\"enabled\"]:\n",
                "    raise ValueError(\"Hybrid strategy is not enabled in config\")\n",
                "\n",
                "reward_strategy = \"hybrid\"  # Default to hybrid strategy\n",
                "cosine_weight = STRATEGY_CONFIG[\"hybrid\"][\"initial_cosine_weight\"]\n",
                "\n",
                "# Initialize database connector\n",
                "try:\n",
                "    db_connector = DatabaseConnector()  # Uses default connection string and db name from config\n",
                "    print(\"Database connection established successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Database connection error: {e}\")\n",
                "    raise\n",
                "\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-007"
            },
            "source": [
                "## 4. Data Loading, Cache Init, Agent/Env Init, and First Reset\n",
                "\n",
                "This combined cell handles:\n",
                "1. Selecting the target candidate ID\n",
                "2. Initializing the TensorCache and loading data from DB\n",
                "3. Initializing the appropriate Environment using the cache\n",
                "4. Initializing the QNetwork, WorldModel, and DynaQAgent using the cache\n",
                "5. Initializing Visualizer and Evaluator\n",
                "6. Performing the first environment reset for the target candidate"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-008"
            },
            "source": [
                "# Get target candidate ID\n",
                "try:\n",
                "    collection = db_connector.db[db_connector.collections[\"candidates_text\"]]\n",
                "    candidate_ids = [doc[\"_id\"] for doc in collection.find({}, {\"_id\": 1}).limit(TRAINING_CONFIG[\"num_candidates\"])]\n",
                "    target_candidate_id = candidate_ids[0]\n",
                "    print(f\"Selected target candidate ID: {target_candidate_id}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error selecting target candidate: {e}\")\n",
                "    raise\n",
                "\n",
                "# Initialize tensor cache\n",
                "CACHE_CONFIG = {\n",
                "    \"enabled\": True,\n",
                "    \"device\": TRAINING_CONFIG.get(\"device\", \"cuda\"),\n",
                "    \"load_all_jobs\": True\n",
                "}\n",
                "\n",
                "tensor_cache = None # Define outside the if block for broader scope if needed later\n",
                "env = None\n",
                "# agent_cosine and agent_hybrid will be defined in their respective training blocks.\n",
                "# The initial agent definition here is mostly for confirming setup and can be streamlined.\n",
                "\n",
                "if CACHE_CONFIG[\"enabled\"]:\n",
                "    print(f\"Initializing tensor cache on {CACHE_CONFIG['device']}...\")\n",
                "    tensor_cache = TensorCache(device=CACHE_CONFIG[\"device\"])\n",
                "    tensor_cache.copy_from_database(\n",
                "        db_connector=db_connector,\n",
                "        applicant_ids=[target_candidate_id]\n",
                "    )\n",
                "    stats = tensor_cache.cache_stats()\n",
                "    # Use the specific counts from stats\n",
                "    print(f\"Cache initialized with {stats['job_count']} jobs and {stats['applicant_state_count']} applicant states.\") \n",
                "    print(f\"Initialization time: {stats['initialization_time']:.2f} seconds\")\n",
                "    print(f\"Memory device: {stats['device']}\")\n",
                "\n",
                "    # Create environment with cache (NO db_connector)\n",
                "    # This env will be used by agent_hybrid. agent_cosine.pretrain uses data directly.\n",
                "    if reward_strategy == \"cosine\": # This path might be less used if hybrid is the focus\n",
                "        env = JobRecommendationEnv(\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"], \n",
                "            reward_strategy=\"cosine\",\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    elif reward_strategy == \"llm\":\n",
                "        env = LLMSimulatorEnv(\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"],\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    elif reward_strategy == \"hybrid\": # This is the expected reward_strategy\n",
                "        env = HybridEnv(\n",
                "            tensor_cache=tensor_cache,\n",
                "            reward_scheme=STRATEGY_CONFIG[\"llm\"][\"response_mapping\"],\n",
                "            cosine_weight=STRATEGY_CONFIG[\"hybrid\"][\"initial_cosine_weight\"],\n",
                "            random_seed=ENV_CONFIG[\"random_seed\"]\n",
                "        )\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown reward strategy: {reward_strategy}\")\n",
                "    print(f\"Created {env.__class__.__name__} for main training (strategy: {reward_strategy}).\")\n",
                "\n",
                "else:\n",
                "    raise RuntimeError(\"TensorCache is disabled in CACHE_CONFIG, but the environment now requires it.\")\n",
                "\n",
                "# Models will be initialized within their respective agent training blocks (9 and 10)\n",
                "# to ensure fresh weights for independent training.\n",
                "\n",
                "# Initialize Visualizer & Evaluator\n",
                "visualizer = Visualizer()\n",
                "evaluator = Evaluator()\n",
                "print(\"Initialized Visualizer and Evaluator.\")\n",
                "\n",
                "# Reset environment for the target candidate (primarily for agent_hybrid training)\n",
                "try:\n",
                "    print(f\"Resetting environment for applicant: {target_candidate_id} (for main training setup)\")\n",
                "    if tensor_cache and target_candidate_id:\n",
                "         print(f\"Cache check before reset: Applicant '{str(target_candidate_id)}' in applicant_states: {str(target_candidate_id) in tensor_cache.applicant_states}\")\n",
                "         \n",
                "    initial_state_for_hybrid_env = env.reset(applicant_id=target_candidate_id) # Store for potential use by hybrid agent\n",
                "    \n",
                "    if not hasattr(env, 'current_state') or env.current_state is None:\n",
                "         raise ValueError(\"Environment reset failed - current_state is None or missing after reset.\")\n",
                "    if initial_state_for_hybrid_env is None:\n",
                "         raise ValueError(\"Environment reset failed - returned None state.\")\n",
                "         \n",
                "    print(f\"Environment reset successful. Initial state shape for hybrid env: {initial_state_for_hybrid_env.shape}\")\n",
                "    print(f\"Environment setup for main agent complete with {reward_strategy} reward strategy.\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"ERROR during env.reset or verification: {e}\")\n",
                "    logger.error(\"Failed during initial environment reset for main agent\", exc_info=True)\n",
                "    raise\n",
                "\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-009"
            },
            "source": [
                "## 5. LLM Integration (If Using LLM-Based Reward Strategy)\n",
                "\n",
                "If we're using an LLM-based reward strategy (either \"llm\" or \"hybrid\"), we need to set up a language model to simulate candidate responses to job recommendations. This helps in generating more realistic rewards based on semantic understanding of both candidate profiles and job descriptions.\n",
                "\n",
                "Note: This section will only execute if we're using an LLM-based strategy and running in a Colab environment with sufficient resources."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-010"
            },
            "source": [
                "# Set up LLM in the environment for reward calculation\n",
                "# This setup applies to the 'env' instance that will be used by agent_hybrid\n",
                "if STRATEGY_CONFIG[\"llm\"][\"enabled\"] and (reward_strategy == \"llm\" or reward_strategy == \"hybrid\"):\n",
                "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "    from huggingface_hub import login\n",
                "    \n",
                "    # Get token path from config and read token\n",
                "    token_path = HF_CONFIG[\"token_path\"]\n",
                "    if not os.path.isabs(token_path):\n",
                "        token_path = token_path\n",
                "    with open(token_path, \"r\") as f:\n",
                "        token = f.read().strip()\n",
                "    \n",
                "    # Login to Hugging Face\n",
                "    login(token=token)\n",
                "    print(f\"Successfully logged in to Hugging Face using token from {token_path}\")\n",
                "    \n",
                "    # Load model and tokenizer\n",
                "    model_id = STRATEGY_CONFIG[\"llm\"][\"model_id\"]\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"load_in_4bit\"],\n",
                "        bnb_4bit_quant_type=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"quant_type\"],\n",
                "        bnb_4bit_use_nested_quant=STRATEGY_CONFIG[\"llm\"][\"quantization\"][\"use_nested_quant\"],\n",
                "        bnb_4bit_compute_dtype=torch.bfloat16 \n",
                "    )\n",
                "    \n",
                "    print(f\"Loading tokenizer for model: {model_id}\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "    \n",
                "    print(f\"Loading model {model_id} with 4-bit quantization...\")\n",
                "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_id,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    \n",
                "    print(f\"LLM model loaded successfully: {model_id}\")\n",
                "    \n",
                "    env = env.setup_llm(llm_model, tokenizer, device=\"auto\") \n",
                "    print(f\"Ensured LLM is set up in {env.__class__.__name__} for strategy '{reward_strategy}' (used by agent_hybrid)\")"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-011"
            },
            "source": [
                "## 8. Training Setup\n",
                "\n",
                "We'll now set up the training process using the DynaQAgent's built-in training interface.\n",
                "The agent handles:\n",
                "1. Experience replay buffer\n",
                "2. Q-Network and World Model updates\n",
                "3. Loss tracking and metrics\n",
                "4. Model checkpointing\n",
                "\n",
                "This section provides general parameters; specific agents will be configured in their training blocks."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-012"
            },
            "source": [
                "# General training parameters that will be used by both agents if not overridden\n",
                "# Print general configs for verification\n",
                "print(f\"Device for training: {device}\")\n",
                "print(f\"Default Planning steps: {TRAINING_CONFIG['planning_steps']}\")\n",
                "print(f\"Default Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
                "print(f\"Default Gamma: {TRAINING_CONFIG['gamma']}\")\n",
                "\n",
                "print(\"General training setup parameters noted.\")"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-013"
            },
            "source": [
                "## 6. Cosine Strategy Agent Training (Baseline)\n",
                "\n",
                "In this phase, we train an agent (`agent_cosine`) using exclusively the cosine similarity for reward generation. This agent will serve as our baseline."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-014"
            },
            "source": [
                "# Define pretraining parameters from config (used for agent_cosine training)\n",
                "num_pretraining_samples = PRETRAINING_CONFIG[\"num_samples\"]\n",
                "num_pretraining_epochs = PRETRAINING_CONFIG[\"num_epochs\"]\n",
                "cosine_training_batch_size = TRAINING_CONFIG[\"batch_size\"] # Can be specific if needed\n",
                "\n",
                "# Initialize agent_cosine with fresh networks\n",
                "print(\"Initializing agent_cosine for COSINE strategy training...\")\n",
                "q_network_cosine = QNetwork(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    hidden_dims=MODEL_CONFIG[\"q_network\"][\"hidden_dims\"],\n",
                "    dropout_rate=MODEL_CONFIG[\"q_network\"][\"dropout_rate\"],\n",
                "    activation=MODEL_CONFIG[\"q_network\"][\"activation\"]\n",
                ").to(device)"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-015"
            },
            "source": [
                "## 7. Hybrid Strategy Agent Training\n",
                "\n",
                "Now we train a separate agent (`agent_hybrid`) using the main experimental strategy (e.g., \"hybrid\").\n",
                "This agent starts with fresh network weights and is trained independently of `agent_cosine`."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-016"
            },
            "source": [
                "world_model_cosine = WorldModel(\n",
                "    input_dim=MODEL_CONFIG[\"world_model\"][\"input_dim\"],\n",
                "    hidden_dims=MODEL_CONFIG[\"world_model\"][\"hidden_dims\"],\n",
                "    dropout_rate=MODEL_CONFIG[\"world_model\"][\"dropout_rate\"],\n",
                "    activation=MODEL_CONFIG[\"world_model\"][\"activation\"]\n",
                ").to(device)\n",
                "\n",
                "agent_cosine = DynaQAgent(\n",
                "    state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "    action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "    q_network=q_network_cosine,\n",
                "    world_model=world_model_cosine,\n",
                "    training_strategy=\"cosine\", # Explicitly cosine\n",
                "    device=device,\n",
                "    target_applicant_id=target_candidate_id,\n",
                "    tensor_cache=tensor_cache\n",
                ")\n",
                "# Configure agent_cosine specific training parameters\n",
                "agent_cosine.planning_steps = TRAINING_CONFIG['planning_steps'] # Or a specific value for cosine agent\n",
                "agent_cosine.batch_size = cosine_training_batch_size\n",
                "agent_cosine.gamma = TRAINING_CONFIG['gamma']\n",
                "print(f\"Initialized agent_cosine with training strategy: {agent_cosine.training_strategy}\")\n",
                "\n",
                "\n",
                "print(\"Starting training for agent_cosine (using pretraining method with cosine data)...\")\n",
                "\n",
                "# Generate pretraining data (cosine-based rewards)\n",
                "print(\"Generating cosine-based training data for agent_cosine...\")\n",
                "cosine_training_data = []\n",
                "\n",
                "try:\n",
                "    initial_state_tensor_cosine = tensor_cache.get_applicant_state(target_candidate_id)\n",
                "except KeyError as e:\n",
                "    print(f\"Error: {e}. Applicant not found in cache. Cannot proceed with agent_cosine training.\")\n",
                "    raise\n",
                "\n",
                "valid_job_indices_cosine = tensor_cache.get_valid_job_indices()\n",
                "print(f\"Found {len(valid_job_indices_cosine)} valid jobs in tensor cache for agent_cosine training\")\n",
                "\n",
                "if not valid_job_indices_cosine:\n",
                "    raise ValueError(\"Tensor cache contains no valid jobs. Cannot generate agent_cosine training data.\")\n",
                "\n",
                "num_samples_to_generate_cosine = min(num_pretraining_samples, len(valid_job_indices_cosine))\n",
                "indices_to_use_cosine = valid_job_indices_cosine[:num_samples_to_generate_cosine]\n",
                "\n",
                "print(\"Pre-calculating all cosine similarity rewards for agent_cosine training...\")\n",
                "all_rewards_cosine = tensor_cache.calculate_cosine_similarities(initial_state_tensor_cosine)\n",
                "if STRATEGY_CONFIG[\"cosine\"][\"scale_reward\"]:\n",
                "    all_rewards_cosine = (all_rewards_cosine + 1) / 2 # Scale from [-1, 1] to [0, 1]\n",
                "print(\"Cosine rewards calculated for agent_cosine.\")\n",
                "    \n",
                "print(f\"Generating {num_samples_to_generate_cosine} samples for agent_cosine training...\")\n",
                "for cache_job_idx_cosine in tqdm(indices_to_use_cosine, desc=\"Generating agent_cosine training data\"):\n",
                "    action_tensor_cosine = tensor_cache.get_job_vector_by_index(cache_job_idx_cosine)\n",
                "    reward_cosine = all_rewards_cosine[cache_job_idx_cosine].item()\n",
                "    next_state_tensor_cosine = initial_state_tensor_cosine \n",
                "    cosine_training_data.append((initial_state_tensor_cosine, action_tensor_cosine, reward_cosine, next_state_tensor_cosine))\n",
                "\n",
                "print(f\"Collected {len(cosine_training_data)} experiences for agent_cosine\")\n",
                "\n",
                "if not cosine_training_data:\n",
                "    raise RuntimeError(\"No training data was collected for agent_cosine. Cannot proceed.\")\n",
                "\n",
                "try:\n",
                "    states_cosine = torch.stack([d[0] for d in cosine_training_data]).to(device)\n",
                "    actions_cosine = torch.stack([d[1] for d in cosine_training_data]).to(device)\n",
                "    rewards_cosine = torch.tensor([d[2] for d in cosine_training_data], dtype=torch.float32, device=device)\n",
                "    next_states_cosine = torch.stack([d[3] for d in cosine_training_data]).to(device)\n",
                "except RuntimeError as e:\n",
                "    print(f\"Error creating tensor batches for agent_cosine: {e}\")\n",
                "    raise\n",
                "\n",
                "# Run training for agent_cosine using the agent's pretrain method (as it takes static data)\n",
                "print(\"Starting agent_cosine network training (using its .pretrain() method)\")\n",
                "agent_cosine_training_metrics = agent_cosine.pretrain(\n",
                "    states=states_cosine,\n",
                "    actions=actions_cosine,\n",
                "    rewards=rewards_cosine,\n",
                "    next_states=next_states_cosine,\n",
                "    num_epochs=num_pretraining_epochs, # Using pretraining epochs for this cosine training\n",
                "    batch_size=cosine_training_batch_size\n",
                ")\n",
                "\n",
                "# Plot agent_cosine training metrics\n",
                "visualizer.plot_training_metrics(\n",
                "    metrics={\n",
                "        'q_losses': agent_cosine_training_metrics['q_losses'],\n",
                "        'world_losses': agent_cosine_training_metrics['world_losses']\n",
                "    },\n",
                "    title=\"agent_cosine Training Losses (Cosine Strategy)\"\n",
                ")\n",
                "\n",
                "print(\"agent_cosine training (using pretraining method) completed successfully.\")"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-017"
            },
            "source": [
                "## 7. Hybrid Strategy Agent Training\n",
                "\n",
                "Now we train a separate agent (`agent_hybrid`) using the main experimental strategy (e.g., \"hybrid\").\n",
                "This agent starts with fresh network weights and is trained independently of `agent_cosine`."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-018"
            },
            "source": [
                "# Training parameters from config for agent_hybrid\n",
                "num_episodes_hybrid = TRAINING_CONFIG[\"num_episodes\"]\n",
                "max_steps_per_episode_hybrid = TRAINING_CONFIG[\"max_steps_per_episode\"]\n",
                "\n",
                "print(f\"Starting main training loop for agent_hybrid (strategy: {reward_strategy})...\")\n",
                "\n",
                "# Set up for multiple experiments for agent_hybrid\n",
                "num_experiments = 5  # Number of experiments to run for agent_hybrid\n",
                "experiment_results_hybrid = {\n",
                "    'q_network_loss': [],\n",
                "    'world_model_loss': [],\n",
                "    'episode_reward': [],\n",
                "    'eval_reward': []\n",
                "}\n",
                "\n",
                "# Ensure the environment for agent_hybrid is the one configured with LLM if hybrid strategy is used.\n",
                "# The 'env' variable should already be the HybridEnv instance if configured earlier.\n",
                "print(f\"Using environment of type: {env.__class__.__name__} for agent_hybrid training.\")\n",
                "\n",
                "# agent_hybrid will be defined and potentially re-initialized inside the loop\n",
                "agent_hybrid = None \n",
                "\n",
                "# Create directory for saving agent_hybrid results\n",
                "results_dir_hybrid = os.path.join(PATH_CONFIG[\"results_dir\"], f\"multi_experiment_{reward_strategy}\")\n",
                "os.makedirs(results_dir_hybrid, exist_ok=True)\n",
                "\n",
                "# Run multiple experiments for agent_hybrid\n",
                "for exp_idx in range(num_experiments):\n",
                "    print(f\"\\nStarting agent_hybrid experiment {exp_idx+1}/{num_experiments}\")\n",
                "    \n",
                "    # Initialize or Reinitialize agent_hybrid for each experiment to ensure independence\n",
                "    print(f\"Initializing new agent_hybrid instance for experiment {exp_idx+1}...\")\n",
                "    q_network_hybrid_exp = QNetwork(\n",
                "        state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "        action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "        hidden_dims=MODEL_CONFIG[\"q_network\"][\"hidden_dims\"],\n",
                "        dropout_rate=MODEL_CONFIG[\"q_network\"][\"dropout_rate\"],\n",
                "        activation=MODEL_CONFIG[\"q_network\"][\"activation\"]\n",
                "    ).to(device)\n",
                "\n",
                "    world_model_hybrid_exp = WorldModel(\n",
                "        input_dim=MODEL_CONFIG[\"world_model\"][\"input_dim\"],\n",
                "        hidden_dims=MODEL_CONFIG[\"world_model\"][\"hidden_dims\"],\n",
                "        dropout_rate=MODEL_CONFIG[\"world_model\"][\"dropout_rate\"],\n",
                "        activation=MODEL_CONFIG[\"world_model\"][\"activation\"]\n",
                "    ).to(device)\n",
                "    \n",
                "    agent_hybrid = DynaQAgent(\n",
                "        state_dim=MODEL_CONFIG[\"q_network\"][\"state_dim\"],\n",
                "        action_dim=MODEL_CONFIG[\"q_network\"][\"action_dim\"],\n",
                "        q_network=q_network_hybrid_exp,\n",
                "        world_model=world_model_hybrid_exp,\n",
                "        training_strategy=reward_strategy, # This should be 'hybrid' or 'llm'\n",
                "        device=device,\n",
                "        target_applicant_id=target_candidate_id,\n",
                "        tensor_cache=tensor_cache\n",
                "    )\n",
                "    # Configure agent_hybrid specific training parameters\n",
                "    agent_hybrid.planning_steps = TRAINING_CONFIG['planning_steps']\n",
                "    agent_hybrid.batch_size = TRAINING_CONFIG['batch_size']\n",
                "    agent_hybrid.gamma = TRAINING_CONFIG['gamma']\n",
                "    print(f\"Initialized agent_hybrid with training strategy: {agent_hybrid.training_strategy}\")\n",
                "\n",
                "    # Run training for this agent_hybrid experiment\n",
                "    # The env here should be the HybridEnv if reward_strategy is hybrid\n",
                "    training_metrics_hybrid = agent_hybrid.train(\n",
                "        env=env, \n",
                "        num_episodes=num_episodes_hybrid,\n",
                "        max_steps_per_episode=max_steps_per_episode_hybrid,\n",
                "        applicant_ids=[target_candidate_id]\n",
                "    )\n",
                "    \n",
                "    # Store results for this agent_hybrid experiment\n",
                "    for key in experiment_results_hybrid:\n",
                "        if key in training_metrics_hybrid:\n",
                "            experiment_results_hybrid[key].append(training_metrics_hybrid[key])\n",
                "    \n",
                "    # Save individual agent_hybrid experiment results\n",
                "    exp_file_hybrid = os.path.join(results_dir_hybrid, f\"experiment_hybrid_{exp_idx+1}.pt\")\n",
                "    torch.save(training_metrics_hybrid, exp_file_hybrid)\n",
                "    print(f\"Saved agent_hybrid experiment {exp_idx+1} results to {exp_file_hybrid}\")\n",
                "\n",
                "# Save aggregated results for agent_hybrid\n",
                "aggregated_file_hybrid = os.path.join(results_dir_hybrid, \"aggregated_results_hybrid.pt\")\n",
                "torch.save(experiment_results_hybrid, aggregated_file_hybrid)\n",
                "print(f\"Saved aggregated agent_hybrid results to {aggregated_file_hybrid}\")\n",
                "\n",
                "# Visualize results for agent_hybrid using the Visualizer\n",
                "visualizer.plot_experiment_results(\n",
                "    experiment_results=experiment_results_hybrid,\n",
                "    title_prefix=f\"Dyna-Q Performance ({reward_strategy.upper()} Strategy)\",\n",
                "    filename_prefix=f\"multi_experiment_{reward_strategy}\"\n",
                ")\n",
                "\n",
                "# Plot training metrics for the last agent_hybrid experiment\n",
                "# Ensure training_metrics_hybrid holds the metrics from the last experiment\n",
                "if training_metrics_hybrid:\n",
                "    visualizer.plot_training_metrics(\n",
                "        metrics={\n",
                "            'q_losses': training_metrics_hybrid['q_network_loss'],\n",
                "            'world_losses': training_metrics_hybrid['world_model_loss'],\n",
                "            'episode_rewards': training_metrics_hybrid['episode_reward']\n",
                "        },\n",
                "        title=f\"Training Metrics (Last {reward_strategy.upper()} Agent Experiment)\"\n",
                "    )\n",
                "\n",
                "print(f\"Multiple experiments for agent_hybrid ({reward_strategy} strategy) completed successfully.\")"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-019"
            },
            "source": [
                "## 7.1 Loading and Visualizing Saved Experiment Results\n",
                "\n",
                "This section provides functionality to load and analyze results from previously run experiments.\n",
                "This is useful for revisiting experiment results without having to rerun the experiments."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-020"
            },
            "source": [
                "def load_experiment_results(results_dir):\n",
                "    \"\"\"\n",
                "    Load saved experiment results for visualization.\n",
                "    \n",
                "    Args:\n",
                "        results_dir: Directory containing experiment results\n",
                "    \n",
                "    Returns:\n",
                "        dict: Loaded experiment results\n",
                "    \"\"\"\n",
                "    # Check if aggregated results exist\n",
                "    aggregated_file = os.path.join(results_dir, \"aggregated_results.pt\")\n",
                "    \n",
                "    if os.path.exists(aggregated_file):\n",
                "        print(f\"Loading aggregated results from {aggregated_file}\")\n",
                "        return torch.load(aggregated_file)\n",
                "    else:\n",
                "        # Try to load individual experiment files\n",
                "        import glob\n",
                "        exp_files = glob.glob(os.path.join(results_dir, \"experiment_*.pt\"))\n",
                "        \n",
                "        if not exp_files:\n",
                "            print(f\"No experiment files found in {results_dir}\")\n",
                "            return {}\n",
                "        \n",
                "        print(f\"Found {len(exp_files)} individual experiment files\")\n",
                "        \n",
                "        # Load individual files and combine them\n",
                "        combined_results = {\n",
                "            'q_network_loss': [],\n",
                "            'world_model_loss': [],\n",
                "            'episode_reward': [],\n",
                "            'eval_reward': []\n",
                "        }\n",
                "        \n",
                "        for file in exp_files:\n",
                "            exp_data = torch.load(file)\n",
                "            for key in combined_results:\n",
                "                if key in exp_data:\n",
                "                    combined_results[key].append(exp_data[key])\n",
                "        \n",
                "        return combined_results\n",
                "\n",
                "# Example usage (commented out by default)\n",
                "# results_dir = os.path.join(PATH_CONFIG[\"results_dir\"], \"multi_experiment\")\n",
                "# loaded_results = load_experiment_results(results_dir)\n",
                "# if loaded_results:\n",
                "#     visualizer.plot_experiment_results(\n",
                "#         experiment_results=loaded_results,\n",
                "#         title_prefix=\"Loaded Results\",\n",
                "#         filename_prefix=\"loaded_experiment\"\n",
                "#     )"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-021"
            },
            "source": [
                "## 8. Evaluation\n",
                "\n",
                "Finally, we evaluate the trained agents: `agent_cosine` (baseline) and `agent_hybrid`.\n",
                "This will:\n",
                "1. Test both agents on a set of evaluation episodes.\n",
                "2. Compare their performance using the `Evaluator`.\n",
                "3. Generate evaluation metrics and visualizations."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-022"
            },
            "source": [
                "# Evaluation parameters from config\n",
                "num_eval_episodes = EVAL_CONFIG[\"num_eval_episodes\"]\n",
                "# baseline_strategy is implicitly \"cosine\" due to agent_cosine's training\n",
                "\n",
                "print(\"Starting evaluation: Comparing agent_cosine vs agent_hybrid...\")\n",
                "\n",
                "# Ensure agent_cosine and agent_hybrid are the trained instances from Block 9 and 10 respectively.\n",
                "# agent_cosine is fully trained after Block 9.\n",
                "# agent_hybrid is the instance from the last iteration of the experiment loop in Block 10.\n",
                "\n",
                "if agent_cosine is None or agent_hybrid is None:\n",
                "    raise RuntimeError(\"One or both agents (agent_cosine, agent_hybrid) are not defined. Ensure training blocks were run.\")\n",
                "\n",
                "# The environment used for evaluation should be consistent for both agents.\n",
                "# Using the 'env' that was set up for the hybrid strategy, as it's more general\n",
                "# or can be adapted by the agents if their internal logic or the evaluator handles it.\n",
                "# If agent_cosine strictly requires a simpler env, that would need specific handling.\n",
                "# However, evaluate_agent primarily uses env.reset, env.get_valid_actions, env.get_action_vector, env.step.\n",
                "# The reward generation within env.step will apply to both if they use the same env instance.\n",
                "# For a fair comparison of learned policies, the evaluation environment should be the same.\n",
                "\n",
                "print(f\"Evaluation will use environment of type: {env.__class__.__name__}\")\n",
                "\n",
                "# Use evaluator's compare_agents method\n",
                "comparison_results = evaluator.compare_agents(\n",
                "    baseline_agent=agent_cosine,    # Agent trained with cosine strategy in Block 9\n",
                "    pretrained_agent=agent_hybrid,  # Agent trained with hybrid strategy in Block 10 (final instance)\n",
                "    env=env,\n",
                "    applicant_ids=[target_candidate_id],\n",
                "    num_episodes=num_eval_episodes\n",
                ")\n",
                "\n",
                "# Extract the results for visualization\n",
                "# Note: 'pretrained' in comparison_results refers to agent_hybrid here\n",
                "evaluation_visualization_data = {\n",
                "    'agent_hybrid_rewards': comparison_results['pretrained']['episode_rewards'],\n",
                "    'agent_cosine_rewards': comparison_results['baseline']['episode_rewards']\n",
                "}\n",
                "\n",
                "# Plot evaluation results\n",
                "visualizer.plot_evaluation_results(\n",
                "    baseline_rewards=evaluation_visualization_data['agent_cosine_rewards'],\n",
                "    pretrained_rewards=evaluation_visualization_data['agent_hybrid_rewards'],\n",
                "    title=\"Evaluation Results: Cosine Agent vs. Hybrid Agent\"\n",
                ")\n",
                "\n",
                "print(\"Evaluation completed successfully.\")"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-023"
            },
            "source": [
                "## 9. Generate Job Recommendations\n",
                "\n",
                "Now let's use our trained `agent_hybrid` to generate personalized job recommendations for our target candidate. We'll:\n",
                "\n",
                "1. Use the trained Q-network of `agent_hybrid` to evaluate jobs\n",
                "2. Select the top-K jobs with highest Q-values\n",
                "3. Display the recommendations along with job details"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-024"
            },
            "source": [
                "# Define testing parameters\n",
                "num_recommendations = EVAL_CONFIG[\"top_k_recommendations\"]\n",
                "# test_epsilon = 0.0 is implicit in agent.select_action with eval_mode=True\n",
                "\n",
                "# Initialize lists to store recommendations\n",
                "recommended_jobs_hybrid = []\n",
                "recommendation_scores_hybrid = []\n",
                "\n",
                "# Get all valid job indices from tensor_cache\n",
                "valid_job_indices_reco = list(range(len(tensor_cache))) # Renamed to avoid conflict\n",
                "print(f\"Found {len(valid_job_indices_reco)} valid jobs in tensor cache for recommendations using agent_hybrid\")\n",
                "\n",
                "# Reset the environment to get the initial state (using the main 'env' instance)\n",
                "state_reco = env.reset(applicant_id=target_candidate_id) # Renamed for clarity\n",
                "\n",
                "# Create a copy of job indices to work with\n",
                "remaining_job_indices_reco = valid_job_indices_reco.copy()\n",
                "\n",
                "if agent_hybrid is None:\n",
                "    raise RuntimeError(\"agent_hybrid is not defined. Ensure training block 10 was run.\")\n",
                "\n",
                "print(f\"Generating top-{num_recommendations} recommendations using agent_hybrid...\")\n",
                "# Generate top-K recommendations using agent_hybrid\n",
                "for _ in range(min(num_recommendations, len(valid_job_indices_reco))):\n",
                "    action_tensors_reco = [tensor_cache.get_job_vector_by_index(idx) for idx in remaining_job_indices_reco]\n",
                "    \n",
                "    if not action_tensors_reco:\n",
                "        print(\"No more actions available to recommend.\")\n",
                "        break\n",
                "        \n",
                "    if not isinstance(state_reco, torch.Tensor):\n",
                "         state_reco = torch.tensor(state_reco, device=device, dtype=torch.float32)\n",
                "         \n",
                "    action_idx_reco, _ = agent_hybrid.select_action(state_reco, action_tensors_reco, eval_mode=True) \n",
                "    \n",
                "    selected_cache_idx_reco = remaining_job_indices_reco[action_idx_reco]\n",
                "    job_id_reco = tensor_cache.get_job_id(selected_cache_idx_reco)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        state_tensor_reco = state_reco.unsqueeze(0) if state_reco.dim() == 1 else state_reco \n",
                "        job_tensor_reco = tensor_cache.get_job_vector_by_index(selected_cache_idx_reco).unsqueeze(0)\n",
                "        q_value_reco = agent_hybrid.q_network(state_tensor_reco, job_tensor_reco).item()\n",
                "    \n",
                "    recommended_jobs_hybrid.append(job_id_reco)\n",
                "    recommendation_scores_hybrid.append(q_value_reco)\n",
                "    \n",
                "    remaining_job_indices_reco.pop(action_idx_reco)\n",
                "\n",
                "# Display recommendations from agent_hybrid\n",
                "print(f\"\\n=== Top Job Recommendations from agent_hybrid ({reward_strategy} strategy) ===\\n\")\n",
                "for i, (job_id, score) in enumerate(zip(recommended_jobs_hybrid, recommendation_scores_hybrid)):\n",
                "    print(f\"Recommendation #{i+1}: [Q-Value: {score:.4f}]\")\n",
                "    print(f\"Job ID: {job_id}\")\n",
                "\n",
                "    try:\n",
                "        job_details = tensor_cache.get_job_metadata(job_id)\n",
                "        print(f\"Title: {job_details.get('job_title', 'N/A')}\")\n",
                "        description = job_details.get('description', 'N/A')\n",
                "        print(f\"Description: {description[:100]}...\" if len(description) > 100 else f\"Description: {description}\")\n",
                "        if 'technical_skills' in job_details:\n",
                "            skills = job_details['technical_skills']\n",
                "            if isinstance(skills, list):\n",
                "                 print(f\"Technical Skills: {', '.join(map(str, skills))}\")\n",
                "            else:\n",
                "                 print(f\"Technical Skills: {skills}\")\n",
                "    except KeyError:\n",
                "        job_id_str = str(job_id)\n",
                "        print(f\"Error: Metadata not found in TensorCache for job {job_id_str}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error retrieving job details from cache: {e}\")\n",
                "    print(\"-\" * 50)"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-025"
            },
            "source": [
                "## 10 Save Final Model Weights (agent_hybrid)\n",
                "\n",
                "This block explicitly saves the state dictionary of the trained Q-network \n",
                "from `agent_hybrid` to the results directory."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-026"
            },
            "source": [
                "# Explicitly save the final Q-network weights of agent_hybrid\n",
                "import os\n",
                "\n",
                "# Define the filename for the saved model\n",
                "try:\n",
                "    model_filename_hybrid = f\"q_network_final_{reward_strategy}_{str(target_candidate_id)}.pt\"\n",
                "except NameError:\n",
                "    model_filename_hybrid = f\"q_network_final_{reward_strategy}_unknown_candidate.pt\"\n",
                "    print(\"Warning: target_candidate_id not found, using default filename for hybrid agent.\")\n",
                "\n",
                "# Get the results directory from PATH_CONFIG (should be results_dir_hybrid or similar)\n",
                "# Using results_dir_hybrid which was defined in Block 10\n",
                "if 'results_dir_hybrid' not in locals():\n",
                "    print(\"Warning: results_dir_hybrid not defined. Attempting to use general results_dir.\")\n",
                "    results_dir_final_save = PATH_CONFIG.get(\"results_dir\", \"../results\")\n",
                "else:\n",
                "    results_dir_final_save = results_dir_hybrid\n",
                "\n",
                "os.makedirs(results_dir_final_save, exist_ok=True)\n",
                "\n",
                "# Construct the full save path\n",
                "save_path_hybrid = os.path.join(results_dir_final_save, model_filename_hybrid)\n",
                "\n",
                "# Save the agent_hybrid's Q-network state dictionary\n",
                "try:\n",
                "    if agent_hybrid and hasattr(agent_hybrid, 'q_network'):\n",
                "        torch.save(agent_hybrid.q_network.state_dict(), save_path_hybrid)\n",
                "        print(f\"Successfully saved agent_hybrid Q-network weights to: {save_path_hybrid}\")\n",
                "    else:\n",
                "        print(\"Error: agent_hybrid or its Q-network not found. Cannot save weights.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error saving agent_hybrid Q-network weights: {e}\")"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cell-027"
            },
            "source": [
                "## 11. Conclusion and Next Steps\n",
                "\n",
                "In this notebook, we've implemented and demonstrated a Neural Dyna-Q job recommendation system. This approach combines the strengths of deep reinforcement learning with model-based planning to provide personalized job recommendations.\n",
                "\n",
                "### 11.1 Key Accomplishments\n",
                "\n",
                "1. **Data Integration**: Connected to the MongoDB database to retrieve real candidate and job data\n",
                "2. **Neural Networks**: Implemented deep Q-network and world model for value function and dynamics prediction\n",
                "3. **Dyna-Q Algorithm**: Combined direct RL with model-based planning for efficient learning\n",
                "4. **Personalized Recommendations**: Generated job recommendations tailored to a specific candidate\n",
                "\n",
                "### 11.2 Potential Improvements\n",
                "\n",
                "1. **Extended Training**: Train for more episodes to improve recommendation quality\n",
                "2. **Hyperparameter Tuning**: Optimize learning rates, network architectures, and other parameters\n",
                "3. **Advanced Reward Functions**: Implement more sophisticated reward strategies using LLMs\n",
                "4. **User Feedback**: Incorporate real user feedback to improve recommendations\n",
                "\n",
                "### 11.3 Applications\n",
                "\n",
                "This system could be deployed as:\n",
                "- A personalized job recommendation service for job seekers\n",
                "- A candidate-job matching tool for recruiters\n",
                "- A component in a larger career guidance system"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "id": "cell-028"
            },
            "source": [
                "# Clean up resources\n",
                "db_connector.close()\n",
                "print(\"Database connection closed.\")\n",
                "\n",
                "# Free up GPU memory\n",
                "if tensor_cache is not None and hasattr(tensor_cache, 'clear'):\n",
                "    tensor_cache.clear()\n",
                "    print(\"Tensor cache cleared.\")\n",
                "    \n",
                "# Free PyTorch memory\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()\n",
                "    print(\"CUDA cache emptied.\")\n",
                "    \n",
                "print(\"Notebook execution complete.\")"
            ],
            "outputs": [],
            "execution_count": null
        }
    ]
}
   