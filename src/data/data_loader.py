"""
Data loader module for preparing and managing training data.
"""

import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional
import logging

# Import configuration
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
from config.config import TRAINING_CONFIG, PRETRAINING_CONFIG

logger = logging.getLogger(__name__)

class ReplayBuffer:
    """
    Replay buffer for storing and sampling experience tuples.
    
    Stores (state, action, reward, next_state) tuples and provides
    methods to sample batches of experiences for training.
    """
    
    def __init__(self, capacity: int = TRAINING_CONFIG["replay_buffer_size"]):
        """
        Initialize the replay buffer.
        
        Args:
            capacity: Maximum number of transitions to store.
        """
        self.capacity = capacity
        self.buffer = []
        self.position = 0
        logger.info(f"Initialized replay buffer with capacity {capacity}")
    
    def push(self, state: torch.Tensor, action: torch.Tensor, reward: float, 
             next_state: torch.Tensor) -> None:
        """
        Add a new experience to the buffer.
        
        Args:
            state: Current state tensor.
            action: Action tensor.
            reward: Reward value.
            next_state: Next state tensor.
        """
        # If buffer is not full, append new experience
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
            
        # Store experience tuple
        self.buffer[self.position] = (state, action, reward, next_state)
        
        # Update position (circular buffer)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Sample a batch of experiences.
        
        Args:
            batch_size: Number of experiences to sample.
            
        Returns:
            Tuple of (states, actions, rewards, next_states) tensors.
        """
        if batch_size > len(self.buffer):
            batch_size = len(self.buffer)
            logger.warning(f"Requested batch size {batch_size} exceeds buffer size {len(self.buffer)}")
            
        # Randomly sample indices
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        
        # Extract experiences
        states, actions, rewards, next_states = zip(*[self.buffer[i] for i in indices])
        
        # Convert to tensors
        states = torch.stack(states)
        actions = torch.stack(actions)
        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)
        next_states = torch.stack(next_states)
        
        return states, actions, rewards, next_states
    
    def __len__(self) -> int:
        """Return the current size of the buffer."""
        return len(self.buffer)


class JobRecommendationDataset(Dataset):
    """
    Dataset for LLM-generated or real pretraining data.
    
    Used during the pretraining phase to load experiences generated by the LLM.
    """
    
    def __init__(self, states: List[torch.Tensor], actions: List[torch.Tensor], 
                 rewards: List[float], next_states: List[torch.Tensor]):
        """
        Initialize the dataset.
        
        Args:
            states: List of state tensors.
            actions: List of action tensors.
            rewards: List of reward values.
            next_states: List of next state tensors.
        """
        assert len(states) == len(actions) == len(rewards) == len(next_states)
        
        self.states = states
        self.actions = actions
        self.rewards = rewards
        self.next_states = next_states
        logger.info(f"Initialized dataset with {len(states)} samples")
    
    def __len__(self) -> int:
        """Return the number of samples in the dataset."""
        return len(self.states)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, float, torch.Tensor]:
        """
        Get a sample from the dataset.
        
        Args:
            idx: Index of the sample.
            
        Returns:
            Tuple of (state, action, reward, next_state).
        """
        return (
            self.states[idx],
            self.actions[idx],
            self.rewards[idx],
            self.next_states[idx]
        )


def create_pretraining_data_loader(dataset: JobRecommendationDataset, 
                                    batch_size: int = PRETRAINING_CONFIG["batch_size"],
                                    shuffle: bool = True,
                                    validation_split: float = PRETRAINING_CONFIG["validation_split"]
                                    ) -> Tuple[DataLoader, Optional[DataLoader]]:
    """
    Create data loaders for pretraining.
    
    Args:
        dataset: JobRecommendationDataset instance.
        batch_size: Batch size for training.
        shuffle: Whether to shuffle the data.
        validation_split: Portion of data to use for validation.
        
    Returns:
        Tuple of (train_loader, val_loader). val_loader is None if validation_split is 0.
    """
    # If no validation split, return just the training loader
    if validation_split == 0:
        return DataLoader(
            dataset, 
            batch_size=batch_size,
            shuffle=shuffle
        ), None
    
    # Calculate split sizes
    dataset_size = len(dataset)
    val_size = int(dataset_size * validation_split)
    train_size = dataset_size - val_size
    
    # Split dataset
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    
    # Create loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=shuffle
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False
    )
    
    logger.info(f"Created data loaders with {train_size} training and {val_size} validation samples")
    return train_loader, val_loader 